{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6844f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import settings_small_hi as s\n",
    "\n",
    "os.environ[\"EXT_DATA_TYPE_FOLDER\"] = s.OUTPUT_POSTFIX.lstrip(\"-\")\n",
    "\n",
    "from common import get_weights\n",
    "from communities import get_communities_spark\n",
    "from features import (\n",
    "    generate_features_spark, generate_features_udf_wrapper, get_edge_features_udf, \n",
    "    SCHEMA_FEAT_UDF, FEATURE_TYPES\n",
    ")\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c7f1e2-c0f3-4464-b223-ddd8c6fef69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/05 18:08:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"artifacts\", ignore_errors=True)\n",
    "\n",
    "config = [\n",
    "    (\"spark.driver.memory\", \"32g\"),\n",
    "    (\"spark.worker.memory\", \"32g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"32g\"),\n",
    "    (\"spark.driver.bindAddress\", \"127.0.0.1\"),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\"),\n",
    "]\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(config))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e69899e-e6a0-4f84-a288-737de0dd3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fdc571c-b31c-4970-8f32-a9955cceb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 7\n",
    "TRAIN_PERC = 0.6\n",
    "VALIDATION_PERC = 0.2\n",
    "TEST_PERC = 0.2\n",
    "\n",
    "NUM_PROCS = 10\n",
    "\n",
    "assert(sum([TRAIN_PERC, VALIDATION_PERC, TEST_PERC]) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b07872dd-5cd5-4a39-9c39-76a9ab296961",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_main = os.path.join(\"features\", os.environ[\"EXT_DATA_TYPE_FOLDER\"])\n",
    "# shutil.rmtree(location_main, ignore_errors=True)\n",
    "\n",
    "location_flow_dispense = f\"{location_main}{os.sep}flow_dispense.parquet\"\n",
    "location_flow_passthrough = f\"{location_main}{os.sep}flow_passthrough.parquet\"\n",
    "location_flow_sink = f\"{location_main}{os.sep}flow_sink.parquet\"\n",
    "\n",
    "location_comm_as_source_features = f\"{location_main}{os.sep}comm_as_source_features.parquet\"\n",
    "location_comm_as_target_features = f\"{location_main}{os.sep}comm_as_target_features.parquet\"\n",
    "location_comm_as_passthrough_features = f\"{location_main}{os.sep}comm_as_passthrough_features.parquet\"\n",
    "location_comm_as_passthrough_features_reverse = f\"{location_main}{os.sep}comm_as_passthrough_features_reverse.parquet\"\n",
    "\n",
    "location_features_node_level = f\"{location_main}{os.sep}features_node_level.parquet\"\n",
    "location_features_edges = f\"{location_main}{os.sep}features_edges.parquet\"\n",
    "\n",
    "location_features_edges_train = f\"{location_main}{os.sep}features_edges_train.parquet\"\n",
    "location_features_edges_valid = f\"{location_main}{os.sep}features_edges_valid.parquet\"\n",
    "location_features_edges_test = f\"{location_main}{os.sep}features_edges_test.parquet\"\n",
    "\n",
    "location_train_trx_features = f\"{location_main}{os.sep}train_trx_features\"\n",
    "location_valid_trx_features = f\"{location_main}{os.sep}valid_trx_features\"\n",
    "location_test_trx_features = f\"{location_main}{os.sep}test_trx_features\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(location_main)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c35a497-3f7f-498a-8a5a-da1a78d1f3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "data = spark.read.parquet(s.STAGED_DATA_LOCATION)\n",
    "data_count_original = data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3a26466-f5b0-4fad-ac12-560d5c671b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/05 18:08:41 WARN TaskSetManager: Stage 8 contains a task of very large size (4543 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 11:================================================>    (184 + 12) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reduced to 91.5%\n",
      "\n",
      "CPU times: user 6.76 s, sys: 160 ms, total: 6.92 s\n",
      "Wall time: 18.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "KEEP_TOP_N = 100\n",
    "\n",
    "data_agg_weights = get_weights(\n",
    "    data.groupby([\"source\", \"target\"])\n",
    "    .agg(\n",
    "        sf.sum(\"amount\").alias(\"amount\")\n",
    "    ).toPandas()\n",
    ")\n",
    "data_agg_weights.sort_values(\"weight\", ascending=False, inplace=True)\n",
    "\n",
    "edges_to_keep = data_agg_weights.groupby(\"source\").head(KEEP_TOP_N).reset_index(drop=True)\n",
    "edges_to_keep.sort_values(\"weight\", ascending=False, inplace=True)\n",
    "edges_to_keep = edges_to_keep.groupby(\"target\").head(KEEP_TOP_N).reset_index(drop=True)\n",
    "edges_to_keep = edges_to_keep.loc[:, [\"source\", \"target\"]].drop_duplicates()\n",
    "edges_to_keep = spark.createDataFrame(edges_to_keep)\n",
    "\n",
    "data_graph = data.join(\n",
    "    edges_to_keep.select(sf.col(\"source\").alias(\"src\"), sf.col(\"target\").alias(\"dst\")),\n",
    "    (sf.col(\"source\") == sf.col(\"src\")) &\n",
    "    (sf.col(\"target\") == sf.col(\"dst\"))\n",
    ").drop(\"src\", \"dst\").persist(StorageLevel.DISK_ONLY)\n",
    "data_count_graph = data_graph.count()\n",
    "reduction = round((data_count_graph / data_count_original) * 100, 2)\n",
    "print(f\"\\nReduced to {reduction}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e156f9b4-e083-498d-b2d1-b1c89e44686c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispense\n",
      "passthrough\n",
      "sink\n",
      "CPU times: user 5min 41s, sys: 16.9 s, total: 5min 58s\n",
      "Wall time: 6min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "left = data_graph.select(\"source\", \"target\", \"timestamp\", \"amount\")\n",
    "select = []\n",
    "for column in left.columns:\n",
    "    select.append(sf.col(column).alias(f\"left_{column}\"))\n",
    "left = left.select(*select)\n",
    "right = data_graph.select(\"source\", \"target\", \"timestamp\", \"amount\")\n",
    "\n",
    "flows_temporal = left.join(\n",
    "    right,\n",
    "    (left[\"left_target\"] == right[\"source\"]) &\n",
    "    (left[\"left_timestamp\"] <= right[\"timestamp\"]),\n",
    "    how=\"inner\"\n",
    ").groupby([\"left_source\", \"left_target\", \"source\", \"target\"]).agg(\n",
    "    sf.sum(\"left_amount\").alias(\"left_amount\"),\n",
    "    sf.sum(\"amount\").alias(\"amount\"),\n",
    ").drop(\"left_target\").select(\n",
    "    sf.col(\"left_source\").alias(\"dispense\"),\n",
    "    sf.col(\"source\").alias(\"passthrough\"),\n",
    "    sf.col(\"target\").alias(\"sink\"),\n",
    "    sf.least(\"left_amount\", \"amount\").alias(\"amount\"),\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "flows_temporal.count()\n",
    "flows_temporal = flows_temporal.toPandas()\n",
    "\n",
    "# TODO: This can be made much faster!\n",
    "flow_dispense, flow_passthrough, flow_sink = [], [], []\n",
    "for flow_data, flow_type in [\n",
    "    (flow_dispense, \"dispense\"), (flow_passthrough, \"passthrough\"), (flow_sink, \"sink\")\n",
    "]:\n",
    "    print(flow_type)\n",
    "    prefix = f\"{s.G_FLOW_PREFIX}{flow_type}_\"\n",
    "    for key, group in flows_temporal.groupby(flow_type):\n",
    "        cycle = group[(group[\"dispense\"] == group[\"sink\"]) & (group[\"dispense\"] != group[\"passthrough\"])]\n",
    "        row = {\n",
    "            \"key\": key,\n",
    "            f\"{prefix}amount_sum\": group[\"amount\"].sum(),\n",
    "            f\"{prefix}amount_mean\": group[\"amount\"].mean(),\n",
    "            f\"{prefix}amount_max\": group[\"amount\"].max(),\n",
    "            f\"{prefix}dispense_count\": group[\"dispense\"].nunique(),\n",
    "            f\"{prefix}passthrough_count\": group[\"passthrough\"].nunique(),\n",
    "            f\"{prefix}sink_count\": group[\"sink\"].nunique(),\n",
    "            f\"{prefix}cycle_sum\": cycle[\"amount\"].sum(),\n",
    "            f\"{prefix}cycle_mean\": cycle[\"amount\"].mean(),\n",
    "            f\"{prefix}cycle_max\": cycle[\"amount\"].max(),\n",
    "            f\"{prefix}cycle_passthrough_count\": cycle[\"passthrough\"].nunique(),\n",
    "        }\n",
    "        flow_data.append(row)\n",
    "\n",
    "pd.DataFrame(flow_dispense).set_index(\"key\").to_parquet(location_flow_dispense)\n",
    "pd.DataFrame(flow_passthrough).set_index(\"key\").to_parquet(location_flow_passthrough)\n",
    "pd.DataFrame(flow_sink).set_index(\"key\").to_parquet(location_flow_sink)\n",
    "\n",
    "del flows_temporal\n",
    "del flow_dispense\n",
    "del flow_passthrough\n",
    "del flow_sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70a13395-4873-4612-b464-1e8103b5ff75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5072693\n",
      "CPU times: user 53.6 ms, sys: 30.4 ms, total: 84 ms\n",
      "Wall time: 3.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trx_ids_sorted = data.sort(\"timestamp\").select(\"transaction_id\").toPandas()[\"transaction_id\"].values\n",
    "trx_count = len(trx_ids_sorted)\n",
    "print(trx_count)\n",
    "\n",
    "last_train_index = int(np.floor(trx_count * TRAIN_PERC))\n",
    "last_validation_index = last_train_index + int(np.floor(trx_count * VALIDATION_PERC))\n",
    "train_indexes = trx_ids_sorted[:last_train_index]\n",
    "validation_indexes = trx_ids_sorted[last_train_index:last_validation_index]\n",
    "test_indexes = trx_ids_sorted[last_validation_index:]\n",
    "\n",
    "train_indexes = spark.createDataFrame(\n",
    "    pd.DataFrame(train_indexes, columns=[\"transaction_id\"])\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "validation_indexes = spark.createDataFrame(\n",
    "    pd.DataFrame(validation_indexes, columns=[\"transaction_id\"])\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "test_indexes = spark.createDataFrame(\n",
    "    pd.DataFrame(test_indexes, columns=[\"transaction_id\"])\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "train = train_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ")\n",
    "validation = validation_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ")\n",
    "test = test_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ")\n",
    "train_validation = train.union(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88213f0e-2497-4f01-b735-f941102e4f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing comm_as_source\n",
      "\n",
      "Processed hop #1 | 609,479 | 304,896\n",
      "Processed hop #2 | 878,611 | 182,007\n",
      "Processed hop #3 | 1,204,789 | 136,313\n",
      "Processed hop #4 | 1,484,369 | 117,205\n",
      "\n",
      "Processing comm_as_target\n",
      "\n",
      "Processed hop #1 | 642,727 | 283,656\n",
      "Processed hop #2 | 2,584,739 | 228,416\n",
      "Processed hop #3 | 3,335,450 | 204,753\n",
      "Processed hop #4 | 4,138,999 | 190,491\n",
      "\n",
      "Processing comm_as_passthrough\n",
      "\n",
      "Processed hop #1 | 504,165 | 211,409\n",
      "Processed hop #2 | 718,202 | 132,477\n",
      "Processed hop #3 | 994,799 | 106,327\n",
      "Processed hop #4 | 1,214,780 | 91,457\n",
      "\n",
      "Processing comm_as_passthrough_reverse\n",
      "\n",
      "Processed hop #1 | 603,456 | 266,303\n",
      "Processed hop #2 | 2,430,787 | 214,752\n",
      "Processed hop #3 | 3,138,668 | 192,637\n",
      "Processed hop #4 | 3,894,875 | 179,453\n",
      "\n",
      "\n",
      "comm_as_source_features\n",
      "\n",
      "CPU times: user 19.1 s, sys: 345 ms, total: 19.5 s\n",
      "Wall time: 19.4 s\n",
      "\n",
      "comm_as_target_features\n",
      "\n",
      "CPU times: user 23.2 s, sys: 329 ms, total: 23.5 s\n",
      "Wall time: 23.5 s\n",
      "\n",
      "comm_as_passthrough_features\n",
      "\n",
      "CPU times: user 14 s, sys: 168 ms, total: 14.2 s\n",
      "Wall time: 14.1 s\n",
      "\n",
      "comm_as_passthrough_features_reverse\n",
      "\n",
      "CPU times: user 21.9 s, sys: 234 ms, total: 22.1 s\n",
      "Wall time: 22 s\n",
      "\n",
      "\n",
      "CPU times: user 2min 43s, sys: 3.64 s, total: 2min 47s\n",
      "Wall time: 2min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_input = data.select(\"*\")\n",
    "nodes_source = set(data.select(\"source\").distinct().toPandas()[\"source\"])\n",
    "nodes_target = set(data.select(\"target\").distinct().toPandas()[\"target\"])\n",
    "nodes_passthrough = nodes_source.intersection(nodes_target)\n",
    "\n",
    "%run generate_flow_features.ipynb\n",
    "\n",
    "comm_as_source_features.to_parquet(location_comm_as_source_features)\n",
    "comm_as_target_features.to_parquet(location_comm_as_target_features)\n",
    "comm_as_passthrough_features.to_parquet(location_comm_as_passthrough_features)\n",
    "comm_as_passthrough_features_reverse.to_parquet(location_comm_as_passthrough_features_reverse)\n",
    "\n",
    "del comm_as_source_features\n",
    "del comm_as_target_features\n",
    "del comm_as_passthrough_features\n",
    "del comm_as_passthrough_features_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48961885-70d5-4fd6-bf88-55694f326092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 862 ms, sys: 287 ms, total: 1.15 s\n",
      "Wall time: 4.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ts_min = data_graph.select(sf.min(\"timestamp\").alias(\"x\")).collect()[0][\"x\"] - timedelta(minutes=1)\n",
    "data_graph_agg = data_graph.groupby([\"source\", \"target\", \"source_bank\", \"target_bank\", \"source_currency\"]).agg(\n",
    "    sf.count(\"source\").alias(\"num_transactions\"),\n",
    "    sf.sum(\"amount\").alias(\"amount\"),\n",
    "    sf.sum(\"source_amount\").alias(\"source_amount\"),\n",
    "    sf.collect_list(sf.array((sf.col(\"timestamp\") - ts_min).cast(\"long\"), sf.col(\"amount\"))).alias(\"timestamps_amounts\"),\n",
    ")\n",
    "data_graph_agg_sdf = data_graph_agg.persist(StorageLevel.DISK_ONLY)\n",
    "_ = data_graph_agg_sdf.count()\n",
    "data_graph_agg = data_graph_agg_sdf.toPandas().convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac9bfb33-be4f-4dbe-a1aa-2f48dc75a2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing communities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.65 s, sys: 252 ms, total: 8.9 s\n",
      "Wall time: 44.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Constructing communities\")\n",
    "\n",
    "in_scope_nodes = list(set(data_graph_agg[\"source\"].unique()).union(data_graph_agg[\"target\"].unique()))\n",
    "window_edges = get_weights(\n",
    "    data_graph_agg.groupby([\"source\", \"target\"]).agg(amount=(\"amount\", \"sum\")).reset_index()\n",
    ")\n",
    "graph = ig.Graph.DataFrame(window_edges, use_vids=False, directed=True)\n",
    "communities = get_communities_spark(in_scope_nodes, graph, NUM_PROCS, spark)\n",
    "\n",
    "del in_scope_nodes\n",
    "del window_edges\n",
    "del graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0941a40-42cd-4a48-ac50-e0c70fada876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communities features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 59s, sys: 25.3 s, total: 4min 24s\n",
      "Wall time: 15min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Communities features creation\")\n",
    "\n",
    "graph = ig.Graph.DataFrame(data_graph_agg, use_vids=False, directed=True)\n",
    "features = generate_features_spark(communities, graph, spark)\n",
    "features.columns = [f\"{s.G_COMM_PREFIX}{x}\" if x != \"key\" else x for x in features.columns]\n",
    "\n",
    "del graph\n",
    "del communities\n",
    "del data_graph_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7093cb9c-2ddb-4172-8a04-d4c2c0924487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-hop-source features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.65 s, sys: 263 ms, total: 2.92 s\n",
      "Wall time: 8min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"1-hop-source features creation\")\n",
    "\n",
    "features_source = data_graph_agg_sdf.withColumn(\"key\", sf.col(\"source\")).groupby(\"key\").applyInPandas(\n",
    "    generate_features_udf_wrapper(False), schema=SCHEMA_FEAT_UDF\n",
    ").toPandas()\n",
    "features_source = pd.DataFrame(features_source[\"features\"].apply(json.loads).tolist())\n",
    "types = {k: v for k, v in FEATURE_TYPES.items() if k in features_source.columns}\n",
    "features_source = features_source.astype(types)\n",
    "features_source.columns = [f\"{s.G_1HOP_PREFIX}{x}\" if x != \"key\" else x for x in features_source.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15e81ebb-1e56-41ba-8a04-9166ad34751c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-hop-target features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.21 s, sys: 286 ms, total: 2.5 s\n",
      "Wall time: 6min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"1-hop-target features creation\")\n",
    "\n",
    "features_target = data_graph_agg_sdf.withColumn(\"key\", sf.col(\"target\")).groupby(\"key\").applyInPandas(\n",
    "    generate_features_udf_wrapper(False), schema=SCHEMA_FEAT_UDF\n",
    ").toPandas()\n",
    "features_target = pd.DataFrame(features_target[\"features\"].apply(json.loads).tolist())\n",
    "types = {k: v for k, v in FEATURE_TYPES.items() if k in features_target.columns}\n",
    "features_target = features_target.astype(types)\n",
    "features_target.columns = [f\"{s.G_1HOP_PREFIX}{x}\" if x != \"key\" else x for x in features_target.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f45f4cf-663d-4fb9-8b90-4064bd2cf3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.33 s, sys: 671 ms, total: 5 s\n",
      "Wall time: 4.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "all_features = features.set_index(\"key\").join(\n",
    "    features_source.set_index(\"key\"), how=\"left\", rsuffix=f\"_1_hop_as_source\"\n",
    ")\n",
    "all_features.index.name = \"key\"\n",
    "all_features = all_features.reset_index()\n",
    "\n",
    "all_features = all_features.set_index(\"key\").join(\n",
    "    features_target.set_index(\"key\"), how=\"left\", rsuffix=f\"_1_hop_as_target\"\n",
    ")\n",
    "\n",
    "all_features = all_features.join(\n",
    "    pd.read_parquet(location_comm_as_source_features), how=\"left\", rsuffix=\"_dispense\"\n",
    ").join(\n",
    "    pd.read_parquet(location_comm_as_target_features), how=\"left\", rsuffix=\"_sink\"\n",
    ").join(\n",
    "    pd.read_parquet(location_comm_as_passthrough_features), how=\"left\", rsuffix=\"_passthrough\"\n",
    ").join(\n",
    "    pd.read_parquet(location_comm_as_passthrough_features_reverse), how=\"left\", rsuffix=\"_passthrough_rev\"\n",
    ").join(\n",
    "    pd.read_parquet(location_flow_dispense), how=\"left\"\n",
    ").join(\n",
    "    pd.read_parquet(location_flow_passthrough), how=\"left\"\n",
    ").join(\n",
    "    pd.read_parquet(location_flow_sink), how=\"left\"\n",
    ")\n",
    "\n",
    "all_features.to_parquet(location_features_node_level)\n",
    "del all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a41180a-7206-47ee-b292-9b2d7fbc8e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.read_parquet(location_features_node_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b7526b8-de67-4aa9-93b5-352a5bb67d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = all_features.loc[:, []]\n",
    "anomalies.loc[:, \"anomaly_score\"] = IsolationForest().fit(\n",
    "    all_features.fillna(0)\n",
    ").decision_function(all_features.fillna(0))\n",
    "anomalies.loc[:, \"anomaly_score\"] += abs(anomalies.loc[:, \"anomaly_score\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c04f2ba-88f3-49d0-b6d8-d6266d19dfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 100.0\n"
     ]
    }
   ],
   "source": [
    "if s.FILE_SIZE == \"Small\":\n",
    "    n_components = 50\n",
    "elif s.FILE_SIZE == \"Medium\":\n",
    "    n_components = 20\n",
    "else:\n",
    "    n_components = 5\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "all_features_dim_reduced = pd.DataFrame(\n",
    "    pca.fit_transform(normalize(all_features.fillna(0), norm=\"l1\", axis=1)),\n",
    "    index=all_features.index\n",
    ")\n",
    "print(n_components, round(sum(pca.explained_variance_ratio_) * 100, 2))\n",
    "all_features_dim_reduced.columns = [\n",
    "    f\"pca_{x + 1}\" for x in all_features_dim_reduced.columns\n",
    "]\n",
    "del all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "240a6967-a4a8-4990-927a-9da791df0c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating edge features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.55 s, sys: 1.84 s, total: 5.39 s\n",
      "Wall time: 3min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(f\"Generating edge features\")\n",
    "\n",
    "to_select = [\"source\", \"target\", \"format\", \"source_currency\", \"source_amount\", \"amount\"]\n",
    "\n",
    "edges_features_input = data.select(to_select).groupby(\n",
    "    [\"source\", \"target\", \"format\", \"source_currency\"]\n",
    ").agg(\n",
    "    sf.sum(\"source_amount\").alias(\"source_amount\"), sf.sum(\"amount\").alias(\"amount\")\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "_ = edges_features_input.count()\n",
    "\n",
    "edge_features = edges_features_input.groupby([\"source\", \"target\"]).applyInPandas(\n",
    "    get_edge_features_udf, schema=SCHEMA_FEAT_UDF\n",
    ").toPandas()\n",
    "edge_features = pd.DataFrame(edge_features[\"features\"].apply(json.loads).tolist())\n",
    "\n",
    "edge_features.to_parquet(location_features_edges)\n",
    "del edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6456cecb-97a0-45c6-bffc-a9db2d004ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_features = pd.read_parquet(location_features_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2884d7dd-1069-4275-b582-66e3604f45db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/05 18:52:12 WARN TaskSetManager: Stage 109 contains a task of very large size (8183 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/05 18:52:13 WARN TaskSetManager: Stage 111 contains a task of very large size (8183 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/05 18:52:18 WARN TaskSetManager: Stage 119 contains a task of very large size (2734 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/05 18:52:18 WARN TaskSetManager: Stage 121 contains a task of very large size (2734 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/05 18:52:21 WARN TaskSetManager: Stage 129 contains a task of very large size (2734 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/05 18:52:22 WARN TaskSetManager: Stage 131 contains a task of very large size (2734 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.45 s, sys: 222 ms, total: 3.67 s\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_edges = train.select(\"source\", \"target\").drop_duplicates().toPandas().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "valid_edges = validation.select(\"source\", \"target\").drop_duplicates().toPandas().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "test_edges = test.select(\"source\", \"target\").drop_duplicates().toPandas().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "\n",
    "train_features = train_edges.join(\n",
    "    edge_features.set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "validation_features = valid_edges.join(\n",
    "    edge_features.set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "test_features = test_edges.join(\n",
    "    edge_features.set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb4a603e-a2af-41bc-8c10-f71c4bae2e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_edge_features(features_in, location):\n",
    "    features_in = features_in.set_index(\"target\").join(\n",
    "        anomalies, how=\"left\"\n",
    "    ).reset_index().set_index(\"source\").join(\n",
    "        anomalies, how=\"left\", rsuffix=\"_source\"\n",
    "    ).reset_index().set_index(\"target\").join(\n",
    "        all_features_dim_reduced, how=\"left\"\n",
    "    ).reset_index().set_index(\"source\").join(\n",
    "        all_features_dim_reduced, how=\"left\", rsuffix=\"_source\"\n",
    "    ).reset_index()\n",
    "    features_in.loc[:, \"anom_scores_diff\"] = features_in.loc[:, \"anomaly_score\"] - features_in.loc[:, \"anomaly_score_source\"]\n",
    "    features_in.loc[:, \"anom_scores_min\"] = np.array(\n",
    "        [\n",
    "            features_in.loc[:, \"anomaly_score\"].values, \n",
    "            features_in.loc[:, \"anomaly_score_source\"].values\n",
    "        ],\n",
    "    ).min(axis=0)\n",
    "    features_in.loc[:, \"anom_scores_max\"] = np.array(\n",
    "        [\n",
    "            features_in.loc[:, \"anomaly_score\"].values, \n",
    "            features_in.loc[:, \"anomaly_score_source\"].values\n",
    "        ],\n",
    "    ).max(axis=0)\n",
    "    features_in.loc[:, \"anom_scores_mean\"] = np.array(\n",
    "        [\n",
    "            features_in.loc[:, \"anomaly_score\"].values, \n",
    "            features_in.loc[:, \"anomaly_score_source\"].values\n",
    "        ],\n",
    "    ).mean(axis=0)\n",
    "    features_in.to_parquet(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7edf80de-4720-4c0a-8d5b-3bfafd0a26c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.9 s, sys: 724 ms, total: 4.63 s\n",
      "Wall time: 4.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "save_edge_features(train_features, location_features_edges_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7adbd114-df75-4bca-b697-7fcd08ae36c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.47 s, sys: 176 ms, total: 1.65 s\n",
      "Wall time: 1.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "save_edge_features(validation_features, location_features_edges_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40d4fed8-cc76-44e0-809a-5d6ae82539c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.96 s, sys: 243 ms, total: 2.21 s\n",
      "Wall time: 2.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "save_edge_features(test_features, location_features_edges_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f4364bc-eb49-4d03-b85b-8ff293a88fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trx_features(data_in, location):\n",
    "    columns = [\"source\", \"target\", \"source_currency\", \"target_currency\", \"format\", \"amount\", \"is_laundering\"]\n",
    "    \n",
    "    trx_features = data_in.select(*columns).toPandas()\n",
    "    trx_features.loc[:, \"inter_currency\"] = trx_features[\"source_currency\"] != trx_features[\"target_currency\"]\n",
    "\n",
    "    trx_features.to_parquet(location)\n",
    "    del trx_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c783f612-4c76-4ea5-aed2-68eeaec1191d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/05 18:52:37 WARN TaskSetManager: Stage 139 contains a task of very large size (8183 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/05 18:52:40 WARN TaskSetManager: Stage 144 contains a task of very large size (2734 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/07/05 18:52:42 WARN TaskSetManager: Stage 149 contains a task of very large size (2734 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.02 s, sys: 276 ms, total: 2.3 s\n",
      "Wall time: 8.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "save_trx_features(train, location_train_trx_features)\n",
    "save_trx_features(validation, location_valid_trx_features)\n",
    "save_trx_features(test, location_test_trx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13c3a17d-14f9-4d34-9103-0c7e2e8c064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(location_features_trx, location_features_edges):\n",
    "    label_columns = [\"source\", \"target\", \"is_laundering\"]\n",
    "    columns_category = [\"source_currency\", \"target_currency\", \"format\"]\n",
    "    new_types = {column: \"category\" for column in columns_category}\n",
    "    new_types.update({\"is_laundering\": bool})\n",
    "    features_input = spark.read.parquet(location_features_edges)\n",
    "    trx_features_input = spark.read.parquet(location_features_trx).withColumn(\n",
    "        \"source_left\", sf.col(\"source\")\n",
    "    ).withColumn(\n",
    "        \"target_left\", sf.col(\"target\")\n",
    "    ).drop(\"source\", \"target\")\n",
    "    features_input = trx_features_input.join(\n",
    "        features_input,\n",
    "        (trx_features_input[\"source_left\"] == features_input[\"source\"]) &\n",
    "        (trx_features_input[\"target_left\"] == features_input[\"target\"]),\n",
    "        how=\"left\"\n",
    "    ).drop(\"source_left\", \"target_left\")\n",
    "    features_input = features_input.toPandas()\n",
    "    return features_input.astype(new_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96867e2d-ceed-4176-b2f2-cb407c592ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/05 18:52:45 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/07/05 18:52:52 ERROR TransportRequestHandler: Error sending result StreamResponse[streamId=1409698681157_0,byteCount=271462243,body=org.apache.spark.storage.BlockManagerManagedBuffer@33cf9e8] to /127.0.0.1:57520; closing connection\n",
      "java.io.IOException: No buffer space available\n",
      "\tat java.base/sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:132)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:97)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:53)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:532)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferFileRegion.transferTo(ChunkedByteBufferFileRegion.scala:60)\n",
      "\tat org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:122)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:368)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel.doWriteInternal(AbstractNioByteChannel.java:238)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel.doWrite0(AbstractNioByteChannel.java:212)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:406)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:929)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.forceFlush(AbstractNioChannel.java:366)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:790)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:732)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/07/05 18:52:52 ERROR TransportRequestHandler: Error sending result RpcResponse[requestId=6891370425530752076,body=NioManagedBuffer[buf=java.nio.HeapByteBuffer[pos=0 lim=13 cap=13]]] to /127.0.0.1:57520; closing connection\n",
      "java.io.IOException: No buffer space available\n",
      "\tat java.base/sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:132)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:97)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:53)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:532)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferFileRegion.transferTo(ChunkedByteBufferFileRegion.scala:60)\n",
      "\tat org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:122)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:368)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel.doWriteInternal(AbstractNioByteChannel.java:238)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel.doWrite0(AbstractNioByteChannel.java:212)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:406)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:929)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.forceFlush(AbstractNioChannel.java:366)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:790)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:732)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/07/05 18:52:52 ERROR TransportRequestHandler: Error sending result RpcResponse[requestId=5854116095471475300,body=NioManagedBuffer[buf=java.nio.HeapByteBuffer[pos=0 lim=13 cap=13]]] to /127.0.0.1:57520; closing connection\n",
      "java.io.IOException: No buffer space available\n",
      "\tat java.base/sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:132)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:97)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:53)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:532)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferFileRegion.transferTo(ChunkedByteBufferFileRegion.scala:60)\n",
      "\tat org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:122)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:368)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel.doWriteInternal(AbstractNioByteChannel.java:238)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel.doWrite0(AbstractNioByteChannel.java:212)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:406)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:929)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.forceFlush(AbstractNioChannel.java:366)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:790)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:732)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/07/05 18:52:52 ERROR TransportResponseHandler: Still have 2 requests outstanding when connection from localhost/127.0.0.1:57272 is closed\n",
      "25/07/05 18:52:52 ERROR OneForOneBlockFetcher: Failed while starting block fetches\n",
      "java.io.IOException: Connection from localhost/127.0.0.1:57272 closed\n",
      "\tat org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:151)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:120)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n",
      "\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n",
      "\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:225)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/07/05 18:52:52 ERROR OneForOneBlockFetcher: Failed while starting block fetches\n",
      "java.io.IOException: Connection from localhost/127.0.0.1:57272 closed\n",
      "\tat org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:151)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:120)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n",
      "\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:280)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n",
      "\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:225)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:303)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1352)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:850)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:811)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.25 s, sys: 7.18 s, total: 10.4 s\n",
      "Wall time: 25.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_features = combine_features(location_train_trx_features, location_features_edges_train)\n",
    "validation_features = combine_features(location_valid_trx_features, location_features_edges_valid)\n",
    "test_features = combine_features(location_test_trx_features, location_features_edges_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2ea56f4-4dba-4cb4-a95e-b8989ca7b917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 506 ms, sys: 263 ms, total: 768 ms\n",
      "Wall time: 768 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "label_columns = [\"source\", \"target\", \"is_laundering\"]\n",
    "\n",
    "missing_columns = (\n",
    "    (set(train_features.columns).symmetric_difference(validation_features.columns)) |\n",
    "    (set(train_features.columns).symmetric_difference(test_features.columns)) |\n",
    "    (set(test_features.columns).symmetric_difference(validation_features.columns))\n",
    ")\n",
    "for column in missing_columns:\n",
    "    if missing in train_features.columns:\n",
    "        print(f\"Deleting missing column from train: {column}\")\n",
    "        del train_features[column]\n",
    "    if missing in validation_features.columns:\n",
    "        print(f\"Deleting missing column from validation: {column}\")\n",
    "        del validation_features[column]\n",
    "    if missing in test_features.columns:\n",
    "        print(f\"Deleting missing column from test: {column}\")\n",
    "        del test_features[column]\n",
    "\n",
    "train_features_labels = train_features.loc[:, label_columns].copy(deep=True)\n",
    "del train_features[\"is_laundering\"]\n",
    "del train_features[\"source\"]\n",
    "del train_features[\"target\"]\n",
    "\n",
    "validation_features_labels = validation_features.loc[:, label_columns].copy(deep=True)\n",
    "validation_features = validation_features.loc[:, train_features.columns]\n",
    "\n",
    "test_features_labels = test_features.loc[:, label_columns].copy(deep=True)\n",
    "test_features = test_features.loc[:, train_features.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0380b412-d2b9-4a75-b514-9b9cc2097209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.0\n"
     ]
    }
   ],
   "source": [
    "print((time.time() - start) // 60)\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db2dbe51-59bf-4e8e-a132-c4c3121ae717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_eval(y, y_):\n",
    "    return 1 - f1_score(y, np.round(y_))\n",
    "\n",
    "\n",
    "def train_model(x, y, x_, y_, cv=False):\n",
    "    if cv:\n",
    "        model = xgb.XGBClassifier(\n",
    "            early_stopping_rounds=50, scale_pos_weight=5,\n",
    "            eval_metric=f1_eval, disable_default_eval_metric=True, num_parallel_tree=1, max_depth=6,\n",
    "            colsample_bytree=1, subsample=1, n_estimators=200,\n",
    "            enable_categorical=True,\n",
    "        )\n",
    "    else:\n",
    "        model = xgb.XGBClassifier(\n",
    "            early_stopping_rounds=20, scale_pos_weight=7,\n",
    "            eval_metric=f1_eval, disable_default_eval_metric=True, \n",
    "            num_parallel_tree=20, max_depth=6,\n",
    "            colsample_bytree=0.5, subsample=0.5, \n",
    "            n_estimators=100, enable_categorical=True,\n",
    "        )\n",
    "    model.fit(x, y, verbose=not cv, eval_set=[(x_, y_)])\n",
    "    print(f\"Best iteration: {model.best_iteration}\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de1642f1-ce3f-4a20-8b99-f0362047c8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-f1_eval:0.96367\n",
      "[1]\tvalidation_0-f1_eval:0.92692\n",
      "[2]\tvalidation_0-f1_eval:0.81088\n",
      "[3]\tvalidation_0-f1_eval:0.67253\n",
      "[4]\tvalidation_0-f1_eval:0.59094\n",
      "[5]\tvalidation_0-f1_eval:0.52533\n",
      "[6]\tvalidation_0-f1_eval:0.50032\n",
      "[7]\tvalidation_0-f1_eval:0.47466\n",
      "[8]\tvalidation_0-f1_eval:0.45476\n",
      "[9]\tvalidation_0-f1_eval:0.44889\n",
      "[10]\tvalidation_0-f1_eval:0.43423\n",
      "[11]\tvalidation_0-f1_eval:0.43335\n",
      "[12]\tvalidation_0-f1_eval:0.43086\n",
      "[13]\tvalidation_0-f1_eval:0.42486\n",
      "[14]\tvalidation_0-f1_eval:0.41975\n",
      "[15]\tvalidation_0-f1_eval:0.42087\n",
      "[16]\tvalidation_0-f1_eval:0.41838\n",
      "[17]\tvalidation_0-f1_eval:0.41477\n",
      "[18]\tvalidation_0-f1_eval:0.40987\n",
      "[19]\tvalidation_0-f1_eval:0.40720\n",
      "[20]\tvalidation_0-f1_eval:0.40741\n",
      "[21]\tvalidation_0-f1_eval:0.40451\n",
      "[22]\tvalidation_0-f1_eval:0.40529\n",
      "[23]\tvalidation_0-f1_eval:0.40243\n",
      "[24]\tvalidation_0-f1_eval:0.40419\n",
      "[25]\tvalidation_0-f1_eval:0.40493\n",
      "[26]\tvalidation_0-f1_eval:0.40517\n",
      "[27]\tvalidation_0-f1_eval:0.40220\n",
      "[28]\tvalidation_0-f1_eval:0.40197\n",
      "[29]\tvalidation_0-f1_eval:0.39924\n",
      "[30]\tvalidation_0-f1_eval:0.39780\n",
      "[31]\tvalidation_0-f1_eval:0.39780\n",
      "[32]\tvalidation_0-f1_eval:0.39693\n",
      "[33]\tvalidation_0-f1_eval:0.39780\n",
      "[34]\tvalidation_0-f1_eval:0.39682\n",
      "[35]\tvalidation_0-f1_eval:0.39715\n",
      "[36]\tvalidation_0-f1_eval:0.39770\n",
      "[37]\tvalidation_0-f1_eval:0.39836\n",
      "[38]\tvalidation_0-f1_eval:0.39890\n",
      "[39]\tvalidation_0-f1_eval:0.39737\n",
      "[40]\tvalidation_0-f1_eval:0.39704\n",
      "[41]\tvalidation_0-f1_eval:0.39537\n",
      "[42]\tvalidation_0-f1_eval:0.39360\n",
      "[43]\tvalidation_0-f1_eval:0.39494\n",
      "[44]\tvalidation_0-f1_eval:0.39471\n",
      "[45]\tvalidation_0-f1_eval:0.39535\n",
      "[46]\tvalidation_0-f1_eval:0.39723\n",
      "[47]\tvalidation_0-f1_eval:0.39723\n",
      "[48]\tvalidation_0-f1_eval:0.39790\n",
      "[49]\tvalidation_0-f1_eval:0.39377\n",
      "[50]\tvalidation_0-f1_eval:0.39610\n",
      "[51]\tvalidation_0-f1_eval:0.39364\n",
      "[52]\tvalidation_0-f1_eval:0.39353\n",
      "[53]\tvalidation_0-f1_eval:0.39340\n",
      "[54]\tvalidation_0-f1_eval:0.39082\n",
      "[55]\tvalidation_0-f1_eval:0.39218\n",
      "[56]\tvalidation_0-f1_eval:0.39057\n",
      "[57]\tvalidation_0-f1_eval:0.39214\n",
      "[58]\tvalidation_0-f1_eval:0.39316\n",
      "[59]\tvalidation_0-f1_eval:0.39033\n",
      "[60]\tvalidation_0-f1_eval:0.39014\n",
      "[61]\tvalidation_0-f1_eval:0.38945\n",
      "[62]\tvalidation_0-f1_eval:0.38808\n",
      "[63]\tvalidation_0-f1_eval:0.38964\n",
      "[64]\tvalidation_0-f1_eval:0.38964\n",
      "[65]\tvalidation_0-f1_eval:0.38964\n",
      "[66]\tvalidation_0-f1_eval:0.39027\n",
      "[67]\tvalidation_0-f1_eval:0.38933\n",
      "[68]\tvalidation_0-f1_eval:0.38801\n",
      "[69]\tvalidation_0-f1_eval:0.38949\n",
      "[70]\tvalidation_0-f1_eval:0.38993\n",
      "[71]\tvalidation_0-f1_eval:0.38923\n",
      "[72]\tvalidation_0-f1_eval:0.38933\n",
      "[73]\tvalidation_0-f1_eval:0.39071\n",
      "[74]\tvalidation_0-f1_eval:0.39116\n",
      "[75]\tvalidation_0-f1_eval:0.39288\n",
      "[76]\tvalidation_0-f1_eval:0.39322\n",
      "[77]\tvalidation_0-f1_eval:0.39514\n",
      "[78]\tvalidation_0-f1_eval:0.39559\n",
      "[79]\tvalidation_0-f1_eval:0.39435\n",
      "[80]\tvalidation_0-f1_eval:0.39593\n",
      "[81]\tvalidation_0-f1_eval:0.39401\n",
      "[82]\tvalidation_0-f1_eval:0.39785\n",
      "[83]\tvalidation_0-f1_eval:0.39910\n",
      "[84]\tvalidation_0-f1_eval:0.39490\n",
      "[85]\tvalidation_0-f1_eval:0.39593\n",
      "[86]\tvalidation_0-f1_eval:0.40361\n",
      "[87]\tvalidation_0-f1_eval:0.40293\n",
      "[88]\tvalidation_0-f1_eval:0.40305\n",
      "Best iteration: 68\n",
      "\n",
      "67.8 56.48\n",
      "\n",
      "CPU times: user 33min 39s, sys: 16.1 s, total: 33min 55s\n",
      "Wall time: 4min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = train_model(\n",
    "    train_features, train_features_labels[\"is_laundering\"].values, \n",
    "    validation_features, validation_features_labels[\"is_laundering\"].values,\n",
    ")\n",
    "y_test_predicted = model.predict(test_features)\n",
    "f1_final = f1_score(test_features_labels[\"is_laundering\"], y_test_predicted) * 100\n",
    "print(\n",
    "    round(f1_final, 2),\n",
    "    round(recall_score(test_features_labels[\"is_laundering\"], y_test_predicted) * 100, 2)\n",
    ")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58e178c2-8cf6-4edb-ae31-320420efc8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Best iteration: 69\n",
      "\n",
      "62.61 51.31\n",
      "Fold 2\n",
      "Best iteration: 58\n",
      "\n",
      "62.98 51.92\n",
      "Fold 3\n",
      "Best iteration: 64\n",
      "\n",
      "63.21 52.2\n",
      "Fold 4\n",
      "Best iteration: 42\n",
      "\n",
      "64.14 53.64\n",
      "Fold 5\n",
      "Best iteration: 71\n",
      "\n",
      "62.69 51.53\n",
      "CPU times: user 23min 11s, sys: 11.3 s, total: 23min 22s\n",
      "Wall time: 3min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "CV_FOLD_PERC = 0.8\n",
    "N_FOLDS = 5\n",
    "\n",
    "f1_scores = []\n",
    "for fold in range(N_FOLDS):\n",
    "    print(\"Fold\", fold + 1)\n",
    "    x_train = train_features.sample(frac=CV_FOLD_PERC)\n",
    "    x_train_labels = x_train.loc[:, []].join(train_features_labels, how=\"left\")\n",
    "    x_validation = validation_features.sample(frac=CV_FOLD_PERC)\n",
    "    x_validation_labels = x_validation.loc[:, []].join(validation_features_labels, how=\"left\")\n",
    "    model = train_model(\n",
    "        x_train, x_train_labels[\"is_laundering\"].values, \n",
    "        x_validation, x_validation_labels[\"is_laundering\"].values,\n",
    "        cv=True\n",
    "    )\n",
    "    y_test_predicted = model.predict(test_features)\n",
    "    f1_cv = f1_score(test_features_labels[\"is_laundering\"], y_test_predicted) * 100\n",
    "    print(\n",
    "        round(f1_cv, 2),\n",
    "        round(recall_score(test_features_labels[\"is_laundering\"], y_test_predicted) * 100, 2)\n",
    "    )\n",
    "    f1_scores.append(f1_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96459de1-0cca-450f-8b61-96192b8bd18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFP best: 64.77  0.47\n"
     ]
    }
   ],
   "source": [
    "print(\"GFP best: 64.77  0.47\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb6bbb2c-8383-43e7-91f2-447c9b013683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.8 0.55\n"
     ]
    }
   ],
   "source": [
    "print(f\"{round(f1_final, 2)} {round(np.std(f1_scores), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7c5a8dc-1b84-4e39-bf01-27b6f39959a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "print((time.time() - start) // 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f57a38-99d5-4a4a-b1a1-3e25e72ad205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
