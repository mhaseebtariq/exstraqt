{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102f290-c8e0-4064-86e8-a89a42e1628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, recall_score, RocCurveDisplay\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "import settings as s\n",
    "\n",
    "os.environ[\"EXSTRAQT_DATA_TYPE_FOLDER\"] = s.OUTPUT_POSTFIX.lstrip(\"-\")\n",
    "\n",
    "from common import get_weights, delete_large_vars, MULTI_PROC_STAGING_LOCATION\n",
    "from communities import get_communities_spark\n",
    "from features import (\n",
    "    generate_features_spark, generate_features_udf_wrapper, get_edge_features_udf,\n",
    "    SCHEMA_FEAT_UDF, CURRENCY_RATES\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01648199-82fc-4bec-8248-828135bd0af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = int(os.environ.get(\"EXSTRAQT_SEED\", 42))\n",
    "print(f\"{SEED=}\")\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35263e16-b057-4ec4-ac78-cc32187428a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXSTRAQT_NUM_PROCS = int(os.environ.get(\"EXSTRAQT_NUM_PROCS\", os.cpu_count()))\n",
    "\n",
    "DIM_REDUCTION_PERC = float(os.environ.get(\"EXSTRAQT_DIM_REDUCTION_PERC\", 1))\n",
    "SCALE_TO_FLOAT_16 = bool(int(os.environ.get(\"EXSTRAQT_SCALE_TO_FLOAT_16\", 0)))\n",
    "SKIP_ANOMALY_DETECTION = bool(int(os.environ.get(\"EXSTRAQT_SKIP_ANOMALY_DETECTION\", 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cbd846-2921-4802-b8b8-38d64092cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    sys.version_info.major,\n",
    "    sys.version_info.minor,\n",
    "    sys.version_info.micro,\n",
    ") != (3, 11, 8):\n",
    "    raise EnvironmentError(\"Only runs efficiently, as tested, on Python 3.11.8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8939c-e2fc-4bfb-ae18-ef1eec4eafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_CONF = [\n",
    "    (\"spark.driver.memory\", \"32g\"),\n",
    "    (\"spark.worker.memory\", \"32g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"32g\"),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\"),\n",
    "    (\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\"),\n",
    "    (\"spark.network.timeout\", \"600s\"),\n",
    "    (\"spark.sql.autoBroadcastJoinThreshold\", -1),\n",
    "    (\"spark.local.dir\", f\".{os.sep}temp-spark\"),\n",
    "]\n",
    "\n",
    "if \"EXSTRAQT_SEED\" in os.environ:\n",
    "    SPARK_CONF.append((\"spark.log.level\", \"ERROR\"))\n",
    "\n",
    "shutil.rmtree(\"artifacts\", ignore_errors=True)\n",
    "shutil.rmtree(\"temp-spark\", ignore_errors=True)\n",
    "spark = (\n",
    "    SparkSession.builder.master(f\"local[{EXSTRAQT_NUM_PROCS}]\").appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(SPARK_CONF))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b1fa7-53c8-4dbb-b25f-795aaca78a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PERC = float(os.environ.get(\"EXSTRAQT_TRAIN_PERC\", 0.60))\n",
    "VALIDATION_PERC = float(os.environ.get(\"EXSTRAQT_VALIDATION_PERC\", 0.20))\n",
    "TEST_PERC = float(os.environ.get(\"EXSTRAQT_TEST_PERC\", 0.20))\n",
    "\n",
    "KEEP_TOP_N = 100\n",
    "\n",
    "assert(sum([TRAIN_PERC, VALIDATION_PERC, TEST_PERC]) == 1)\n",
    "\n",
    "location_main = os.path.join(\"features\", os.environ[\"EXSTRAQT_DATA_TYPE_FOLDER\"])\n",
    "# shutil.rmtree(location_main, ignore_errors=True)\n",
    "\n",
    "location_communities_leiden = f\"{location_main}{os.sep}communities_leiden.parquet\"\n",
    "\n",
    "location_features_leiden = f\"{location_main}{os.sep}features_leiden.parquet\"\n",
    "location_features_ego = f\"{location_main}{os.sep}features_ego.parquet\"\n",
    "location_features_2_hop = f\"{location_main}{os.sep}features_2_hop.parquet\"\n",
    "location_features_2_hop_out = f\"{location_main}{os.sep}features_2_hop_out.parquet\"\n",
    "location_features_2_hop_in = f\"{location_main}{os.sep}features_2_hop_in.parquet\"\n",
    "location_features_2_hop_combined = f\"{location_main}{os.sep}features_2_hop_combined.parquet\"\n",
    "location_features_source = f\"{location_main}{os.sep}features_source.parquet\"\n",
    "location_features_target = f\"{location_main}{os.sep}features_target.parquet\"\n",
    "\n",
    "location_flow_dispense = f\"{location_main}{os.sep}flow_dispense.parquet\"\n",
    "location_flow_passthrough = f\"{location_main}{os.sep}flow_passthrough.parquet\"\n",
    "location_flow_sink = f\"{location_main}{os.sep}flow_sink.parquet\"\n",
    "\n",
    "location_comm_as_source_features = f\"{location_main}{os.sep}comm_as_source_features.parquet\"\n",
    "location_comm_as_target_features = f\"{location_main}{os.sep}comm_as_target_features.parquet\"\n",
    "location_comm_as_passthrough_features = f\"{location_main}{os.sep}comm_as_passthrough_features.parquet\"\n",
    "location_comm_as_passthrough_features_reverse = f\"{location_main}{os.sep}comm_as_passthrough_features_reverse.parquet\"\n",
    "\n",
    "location_features_node_level = f\"{location_main}{os.sep}features_node_level.parquet\"\n",
    "location_features_edges = f\"{location_main}{os.sep}features_edges.parquet\"\n",
    "\n",
    "location_features_edges_train = f\"{location_main}{os.sep}features_edges_train.parquet\"\n",
    "location_features_edges_valid = f\"{location_main}{os.sep}features_edges_valid.parquet\"\n",
    "location_features_edges_test = f\"{location_main}{os.sep}features_edges_test.parquet\"\n",
    "\n",
    "location_train_trx_features = f\"{location_main}{os.sep}train_trx_features.parquet\"\n",
    "location_valid_trx_features = f\"{location_main}{os.sep}valid_trx_features.parquet\"\n",
    "location_test_trx_features = f\"{location_main}{os.sep}test_trx_features.parquet\"\n",
    "\n",
    "location_train_features = f\"{location_main}{os.sep}train_features.parquet\"\n",
    "location_valid_features = f\"{location_main}{os.sep}valid_features.parquet\"\n",
    "location_test_features = f\"{location_main}{os.sep}test_features.parquet\"\n",
    "\n",
    "location_train_features_dm = f\"{location_main}{os.sep}train_dm.bin\"\n",
    "location_valid_features_dm = f\"{location_main}{os.sep}valid_dm.bin\"\n",
    "location_test_features_dm = f\"{location_main}{os.sep}test_dm.bin\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(location_main)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82546745-6577-4e38-bf44-1a6114c3a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet(s.STAGED_DATA_LOCATION)\n",
    "data = data.withColumn(\"is_laundering\", sf.col(\"is_laundering\").cast(\"boolean\"))\n",
    "data_count_original = data.count()\n",
    "\n",
    "# Also not used in the benchmarks\n",
    "data = data.drop(\"source_entity\", \"target_entity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5d7e4-1b66-4e93-b67b-a947d8c8e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trx_ids_sorted = data.sort(\"timestamp\").select(\"transaction_id\").toPandas()[\"transaction_id\"].values\n",
    "trx_count = len(trx_ids_sorted)\n",
    "\n",
    "last_train_index = int(np.floor(trx_count * TRAIN_PERC))\n",
    "last_validation_index = last_train_index + int(np.floor(trx_count * VALIDATION_PERC))\n",
    "train_indexes = trx_ids_sorted[:last_train_index]\n",
    "validation_indexes = trx_ids_sorted[last_train_index:last_validation_index]\n",
    "test_indexes = trx_ids_sorted[last_validation_index:]\n",
    "\n",
    "train_indexes_loc = os.path.join(location_main, \"temp_train_indexes.parquet\")\n",
    "validation_indexes_loc = os.path.join(location_main, \"temp_validation_indexes.parquet\")\n",
    "test_indexes_loc = os.path.join(location_main, \"temp_test_indexes.parquet\")\n",
    "\n",
    "pd.DataFrame(train_indexes, columns=[\"transaction_id\"]).to_parquet(train_indexes_loc)\n",
    "pd.DataFrame(validation_indexes, columns=[\"transaction_id\"]).to_parquet(validation_indexes_loc)\n",
    "pd.DataFrame(test_indexes, columns=[\"transaction_id\"]).to_parquet(test_indexes_loc)\n",
    "\n",
    "train_indexes = spark.read.parquet(train_indexes_loc)\n",
    "validation_indexes = spark.read.parquet(validation_indexes_loc)\n",
    "test_indexes = spark.read.parquet(test_indexes_loc)\n",
    "\n",
    "train = train_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "validation = validation_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "test = test_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "train_count, validation_count, test_count = train.count(), validation.count(), test.count()\n",
    "print()\n",
    "print(trx_count, train_count, validation_count, test_count)\n",
    "print()\n",
    "\n",
    "os.remove(train_indexes_loc)\n",
    "os.remove(validation_indexes_loc)\n",
    "os.remove(test_indexes_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d1078e-cc6f-4574-a729-28d81fb4fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edge_features(input_data):\n",
    "    print(f\"Generating edge features\")\n",
    "    to_select = [\"source\", \"target\", \"format\", \"source_currency\", \"source_amount\", \"amount\", \"timestamp\"]\n",
    "    edges_features_input = input_data.select(*to_select).groupby(\n",
    "        [\"source\", \"target\", \"format\", \"source_currency\"]\n",
    "    ).agg(\n",
    "        sf.sum(\"source_amount\").alias(\"source_amount\"), \n",
    "        sf.sum(\"amount\").alias(\"amount\"),\n",
    "        sf.unix_timestamp(sf.min(\"timestamp\")).alias(\"min_ts\"),\n",
    "        sf.unix_timestamp(sf.max(\"timestamp\")).alias(\"max_ts\"),\n",
    "    ).repartition(os.cpu_count() * 2, \"source\", \"target\").persist(StorageLevel.DISK_ONLY)\n",
    "    _ = edges_features_input.count()\n",
    "    edge_features = edges_features_input.groupby([\"source\", \"target\"]).applyInPandas(\n",
    "        get_edge_features_udf, schema=SCHEMA_FEAT_UDF\n",
    "    ).toPandas()\n",
    "    edge_features = pd.DataFrame(edge_features[\"features\"].apply(json.loads).tolist())\n",
    "    edge_features.to_parquet(location_features_edges)\n",
    "    del edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68e646-67f2-4158-ba72-528c94207d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_node_features_to_edges(features_in, location):\n",
    "    features_in = features_in.set_index(\"target\").join(\n",
    "        pd.read_parquet(location_features_node_level), how=\"left\", rsuffix=\"_target\"\n",
    "    ).reset_index().set_index(\"source\").join(\n",
    "        pd.read_parquet(location_features_node_level), how=\"left\", rsuffix=\"_source\"\n",
    "    ).reset_index()\n",
    "\n",
    "    if \"anomaly_score\" in features_in.columns:\n",
    "        features_in.loc[:, \"anomaly_scores_diff\"] = features_in.loc[:, \"anomaly_score\"] - features_in.loc[:, \"anomaly_score_source\"]\n",
    "        features_in.loc[:, \"anomaly_scores_min\"] = np.array(\n",
    "            [\n",
    "                features_in.loc[:, \"anomaly_score\"].values, \n",
    "                features_in.loc[:, \"anomaly_score_source\"].values\n",
    "            ],\n",
    "        ).min(axis=0)\n",
    "        features_in.loc[:, \"anomaly_scores_max\"] = np.array(\n",
    "            [\n",
    "                features_in.loc[:, \"anomaly_score\"].values, \n",
    "                features_in.loc[:, \"anomaly_score_source\"].values\n",
    "            ],\n",
    "        ).max(axis=0)\n",
    "        features_in.loc[:, \"anomaly_scores_mean\"] = np.array(\n",
    "            [\n",
    "                features_in.loc[:, \"anomaly_score\"].values, \n",
    "                features_in.loc[:, \"anomaly_score_source\"].values\n",
    "            ],\n",
    "        ).mean(axis=0)\n",
    "    else:\n",
    "        print(\"`anomaly_score` calculations missing in the nodes features!\")\n",
    "\n",
    "    features_in.to_parquet(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4364bc-eb49-4d03-b85b-8ff293a88fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trx_features(data_in, location):\n",
    "    columns = [\n",
    "        \"source\", \"target\", \"source_currency\", \"target_currency\", \"format\", \"amount\", \n",
    "        \"source_dispensation\",\n",
    "        \"target_accumulation\",\n",
    "        \"source_positive_balance\",\n",
    "        \"source_negative_balance\",\n",
    "        \"target_positive_balance\",\n",
    "        \"target_negative_balance\",\n",
    "        \"source_active_for\",\n",
    "        \"target_active_for\",\n",
    "        \"is_laundering\"\n",
    "    ]\n",
    "    missing_columns = set(columns) - set(data_in.columns)\n",
    "    if missing_columns:\n",
    "        print(f\"Skipping the missing transaction columns: {sorted(missing_columns)}\")\n",
    "    columns_common = list(set(columns).intersection(data_in.columns))\n",
    "    trx_features = data_in.select(*columns_common).toPandas()\n",
    "    if \"source_positive_balance\" in columns_common:\n",
    "        trx_features.loc[:, \"source_balance_ratio\"] = (\n",
    "            trx_features[\"source_positive_balance\"] / trx_features[\"source_negative_balance\"]\n",
    "        ).fillna(0).replace(np.inf, 0)\n",
    "        trx_features.loc[:, \"target_balance_ratio\"] = (\n",
    "            trx_features[\"target_positive_balance\"] / trx_features[\"target_negative_balance\"]\n",
    "        ).fillna(0).replace(np.inf, 0)\n",
    "    trx_features.loc[:, \"inter_currency\"] = trx_features[\"source_currency\"] != trx_features[\"target_currency\"]\n",
    "    trx_features.to_parquet(location)\n",
    "    del trx_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596039be-5927-4c81-b14e-e56d1f9073c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_up_memory(to_keep_in, locals_in):\n",
    "    to_reset = %who_ls\n",
    "    to_reset = list(to_reset)\n",
    "    if \"to_keep\" in to_reset:\n",
    "        to_reset.remove(\"to_keep\")\n",
    "    to_reset = set(to_reset) - set(to_keep_in)\n",
    "    for var_to_reset in list(to_reset):\n",
    "        var_to_reset = f\"^{var_to_reset}$\"\n",
    "        %reset_selective -f {var_to_reset}\n",
    "    \n",
    "    delete_large_vars(globals(), locals_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb171cd6-c01d-4bd2-8a32-8d04f50db5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later on, we will reset the variables (to free up memory), while still keeping these intact\n",
    "to_keep = %who_ls\n",
    "to_keep = list(to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c145f2-5598-4551-b0c1-497cb1f1ae41",
   "metadata": {},
   "source": [
    "# [To prevent data leakage]\n",
    "\n",
    "### As the `train`, `validation`, and `test` sets are split in chronological order:\n",
    "* `train` features are constructed, based on a **graph** (containing data), up till the last training record\n",
    "* `validation` features are constructed, ..., up till the last validation record\n",
    "* `train` features are constructed, ..., up till the last test record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4ecbd-1487-4b07-b7c6-1fb22e3bf4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data = train.select(\"*\")\n",
    "print(f\"Constructing node-level features for `train` data: {data.count():,}\")\n",
    "\n",
    "%run node_level_features.ipynb\n",
    "\n",
    "generate_edge_features(data)\n",
    "train_edges = train.select(\"source\", \"target\").drop_duplicates().toPandas().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "train_features = train_edges.join(\n",
    "    pd.read_parquet(location_features_edges).set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "add_node_features_to_edges(train_features, location_features_edges_train)\n",
    "save_trx_features(train, location_train_trx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026f90e0-be1b-4891-b0ed-329d3aa24d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_up_memory(to_keep, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e1408-4d7a-4547-baaa-0cac6952106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data = train.union(validation).select(\"*\")\n",
    "print(f\"Constructing node-level features for `validation` data: {data.count():,}\")\n",
    "\n",
    "%run node_level_features.ipynb\n",
    "\n",
    "generate_edge_features(data)\n",
    "validation_edges = validation.select(\"source\", \"target\").drop_duplicates().toPandas().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "validation_features = validation_edges.join(\n",
    "    pd.read_parquet(location_features_edges).set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "add_node_features_to_edges(validation_features, location_features_edges_valid)\n",
    "save_trx_features(validation, location_valid_trx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d6ec91-1857-43c6-8765-bbaecb1dd226",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_up_memory(to_keep, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec78fa4-1bb2-4233-afe9-e7e445302c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data = train.union(validation).union(test).select(\"*\")\n",
    "print(f\"Constructing node-level features for `test` data: {data.count():,}\")\n",
    "\n",
    "%run node_level_features.ipynb\n",
    "\n",
    "generate_edge_features(data)\n",
    "test_edges = test.select(\"source\", \"target\").drop_duplicates().toPandas().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "test_features = test_edges.join(\n",
    "    pd.read_parquet(location_features_edges).set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "add_node_features_to_edges(test_features, location_features_edges_test)\n",
    "save_trx_features(test, location_test_trx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d6b5e-e38a-45c7-9892-44ae4e5150d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_up_memory(to_keep, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c3a17d-14f9-4d34-9103-0c7e2e8c064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(location_features_trx, location_features_edges, location_features):\n",
    "    features_input = spark.read.parquet(location_features_edges)\n",
    "    trx_features_input = spark.read.parquet(location_features_trx)\n",
    "    features_input = trx_features_input.join(\n",
    "        features_input,\n",
    "        on=[\"source\", \"target\"],\n",
    "        how=\"left\"\n",
    "    ).drop(\"source\", \"target\")\n",
    "    features_input.repartition(12).write.parquet(location_features, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96867e2d-ceed-4176-b2f2-cb407c592ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "combine_features(location_train_trx_features, location_features_edges_train, location_train_features)\n",
    "combine_features(location_valid_trx_features, location_features_edges_valid, location_valid_features)\n",
    "combine_features(location_test_trx_features, location_features_edges_test, location_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8924af3-4f88-48dc-9e82-ab345e77f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(MULTI_PROC_STAGING_LOCATION, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a0e4a-cde2-4963-9e8c-1393f327b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_types(input_part_df):\n",
    "    types = {\"related_for\": np.uint32}\n",
    "    for key, value in input_part_df.dtypes.to_dict().items():\n",
    "        if key.startswith(\"fa_\"):\n",
    "            types[key] = np.float16 if SCALE_TO_FLOAT_16 else np.float32\n",
    "        elif value == np.dtype(\"O\"):\n",
    "            types[key] = \"category\"\n",
    "        elif value == np.float64:\n",
    "            types[key] = np.float32\n",
    "    return types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f95431-13fc-4784-b3f0-15eca4928aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = sorted(\n",
    "    set(spark.read.parquet(location_train_features).columns) &\n",
    "    set(spark.read.parquet(location_valid_features).columns) &\n",
    "    set(spark.read.parquet(location_test_features).columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60ccdde-b078-4c42-93d4-76cad810042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_train = spark.read.parquet(location_train_features).where(\n",
    "    sf.col(\"is_laundering\")\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f811e405-e62d-4475-99ec-24ed175ff8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "del spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454a2a19-c170-4298-bdd4-84fa19de31e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_dmatrix(\n",
    "    location_input_features, columns_to_keep, out_location, \n",
    "    split_at=-1, positives=None\n",
    "):\n",
    "    if split_at > 0:\n",
    "        shutil.rmtree(out_location, ignore_errors=True)\n",
    "        try:\n",
    "            os.makedirs(out_location)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "    input_features = pd.DataFrame()\n",
    "    count = 0\n",
    "    for fl in glob(f\"{location_input_features}{os.sep}*.parquet\"):\n",
    "        count += 1\n",
    "        inner = pd.read_parquet(fl, columns=columns_to_keep)\n",
    "        if positives is not None:\n",
    "            inner = inner.loc[~inner[\"is_laundering\"], :]\n",
    "            inner = pd.concat([inner, positives], ignore_index=True)\n",
    "        inner = inner.astype(get_new_types(inner))\n",
    "        input_features = pd.concat([input_features, inner], ignore_index=True)\n",
    "        if (split_at > 0) and not(count % split_at):\n",
    "            del inner\n",
    "            input_features_labels = input_features.loc[:, [\"is_laundering\"]].copy(deep=True)\n",
    "            del input_features[\"is_laundering\"]\n",
    "            input_dm = xgb.DMatrix(data=input_features, label=input_features_labels, enable_categorical=True)\n",
    "            del input_features\n",
    "            del input_features_labels\n",
    "            input_dm.save_binary(f\"{out_location}{os.sep}{count}.bin\")\n",
    "            del input_dm\n",
    "            input_features = pd.DataFrame()\n",
    "    if not input_features.empty:\n",
    "        del inner\n",
    "        input_features_labels = input_features.loc[:, [\"is_laundering\"]].copy(deep=True)\n",
    "        del input_features[\"is_laundering\"]\n",
    "        input_dm = xgb.DMatrix(data=input_features, label=input_features_labels, enable_categorical=True)\n",
    "        del input_features\n",
    "        del input_features_labels\n",
    "        if split_at > 0:\n",
    "            input_dm.save_binary(f\"{out_location}{os.sep}{count}.bin\")\n",
    "        else:\n",
    "            input_dm.save_binary(out_location)\n",
    "        del input_dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2e7fa9-dccc-4d53-8c39-32244715b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "split_at = 12\n",
    "if s.FILE_SIZE == \"Medium\":\n",
    "    split_at = 6\n",
    "elif s.FILE_SIZE == \"Large\":\n",
    "    split_at = 3\n",
    "\n",
    "positives_train = None\n",
    "\n",
    "save_as_dmatrix(\n",
    "    location_train_features, columns_to_keep, location_train_features_dm, \n",
    "    split_at=split_at, positives=positives_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0e279-9442-4365-ae11-f1af13b63110",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "save_as_dmatrix(location_valid_features, columns_to_keep, location_valid_features_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d02fe-bbbb-4167-92f4-245cb50a3ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "save_as_dmatrix(location_test_features, columns_to_keep, location_test_features_dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
