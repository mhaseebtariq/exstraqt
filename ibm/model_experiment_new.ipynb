{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6844f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from glob import glob\n",
    "from datetime import timedelta, datetime\n",
    "from itertools import combinations\n",
    "\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import settings as s\n",
    "from common import create_workload_for_multi_proc, get_weights\n",
    "from communities import get_communities_multi_proc\n",
    "from features import get_features_multi_proc, get_edge_features\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c7f1e2-c0f3-4464-b223-ddd8c6fef69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/02 21:07:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "config = [\n",
    "    (\"spark.driver.memory\", \"16g\"),\n",
    "    (\"spark.worker.memory\", \"16g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"16g\"),\n",
    "    (\"spark.driver.bindAddress\", \"127.0.0.1\"),\n",
    "    # (\"spark.sql.autoBroadcastJoinThreshold\", \"2g\"),\n",
    "]\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(config))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e69899e-e6a0-4f84-a288-737de0dd3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fdc571c-b31c-4970-8f32-a9955cceb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 7\n",
    "TRAIN_PERC = 0.6\n",
    "VALIDATION_PERC = 0.2\n",
    "TEST_PERC = 0.2\n",
    "\n",
    "NUM_PROCS = 10\n",
    "\n",
    "assert(sum([TRAIN_PERC, VALIDATION_PERC, TEST_PERC]) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "256cccdc-3a06-45c0-a132-028e839a3ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pandas(df):\n",
    "    df.write.parquet(\"temp.parquet\", mode=\"overwrite\")\n",
    "    df = pd.read_parquet(\"temp.parquet\")\n",
    "    if \"timestamp\" in df.columns:\n",
    "        # Because of tz discrepancy\n",
    "        df.loc[:, \"timestamp\"] += timedelta(hours=2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c35a497-3f7f-498a-8a5a-da1a78d1f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet(s.STAGED_DATA_LOCATION)\n",
    "data_count_original = data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3a26466-f5b0-4fad-ac12-560d5c671b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/02 21:09:50 WARN TaskSetManager: Stage 7 contains a task of very large size (16227 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 14:===========================================>         (163 + 12) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reduced to 90.53%\n",
      "\n",
      "CPU times: user 1min 49s, sys: 1.15 s, total: 1min 50s\n",
      "Wall time: 3min 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "KEEP_TOP_N = 100\n",
    "\n",
    "data_agg_weights = get_weights(\n",
    "    get_pandas(\n",
    "        data.groupby([\"source\", \"target\"])\n",
    "        .agg(\n",
    "            sf.sum(\"amount\").alias(\"amount\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "data_agg_weights.sort_values(\"weight\", ascending=False, inplace=True)\n",
    "\n",
    "edges_to_keep = data_agg_weights.groupby(\"source\").head(KEEP_TOP_N).reset_index(drop=True)\n",
    "edges_to_keep.sort_values(\"weight\", ascending=False, inplace=True)\n",
    "edges_to_keep = edges_to_keep.groupby(\"target\").head(KEEP_TOP_N).reset_index(drop=True)\n",
    "edges_to_keep = edges_to_keep.loc[:, [\"source\", \"target\"]].drop_duplicates()\n",
    "edges_to_keep = spark.createDataFrame(edges_to_keep)\n",
    "\n",
    "data_graph = data.join(\n",
    "    edges_to_keep.select(sf.col(\"source\").alias(\"src\"), sf.col(\"target\").alias(\"dst\")),\n",
    "    (sf.col(\"source\") == sf.col(\"src\")) &\n",
    "    (sf.col(\"target\") == sf.col(\"dst\"))\n",
    ").drop(\"src\", \"dst\").persist(StorageLevel.DISK_ONLY)\n",
    "data_count_graph = data_graph.count()\n",
    "reduction = round((data_count_graph / data_count_original) * 100, 2)\n",
    "print(f\"\\nReduced to {reduction}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e156f9b4-e083-498d-b2d1-b1c89e44686c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=====================================================>(199 + 1) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 209 ms, sys: 77.5 ms, total: 287 ms\n",
      "Wall time: 4min 28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "28558658"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "left = data_graph.select(\"source\", \"target\", \"timestamp\", \"amount\")\n",
    "select = []\n",
    "for column in left.columns:\n",
    "    select.append(sf.col(column).alias(f\"left_{column}\"))\n",
    "left = left.select(*select)\n",
    "right = data_graph.select(\"source\", \"target\", \"timestamp\", \"amount\")\n",
    "\n",
    "flows_temporal = left.join(\n",
    "    right,\n",
    "    (left[\"left_target\"] == right[\"source\"]) &\n",
    "    (left[\"left_timestamp\"] <= right[\"timestamp\"]),\n",
    "    how=\"inner\"\n",
    ").groupby([\"left_source\", \"left_target\", \"source\", \"target\"]).agg(\n",
    "    sf.sum(\"left_amount\").alias(\"left_amount\"),\n",
    "    sf.sum(\"amount\").alias(\"amount\"),\n",
    ").drop(\"left_target\").persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "flows_temporal.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b936c12-027a-4cec-8360-76fb92def8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179504480\n"
     ]
    }
   ],
   "source": [
    "trx_ids_sorted = get_pandas(data.sort(\"timestamp\").select(\"transaction_id\"))[\"transaction_id\"].values\n",
    "trx_count = len(trx_ids_sorted)\n",
    "print(trx_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70a13395-4873-4612-b464-1e8103b5ff75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107702688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35900896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35900896\n",
      "\n",
      "CPU times: user 175 ms, sys: 237 ms, total: 413 ms\n",
      "Wall time: 32.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "last_train_index = int(np.floor(trx_count * TRAIN_PERC))\n",
    "last_validation_index = last_train_index + int(np.floor(trx_count * VALIDATION_PERC))\n",
    "train_indexes = trx_ids_sorted[:last_train_index]\n",
    "validation_indexes = trx_ids_sorted[last_train_index:last_validation_index]\n",
    "test_indexes = trx_ids_sorted[last_validation_index:]\n",
    "\n",
    "pd.DataFrame(train_indexes, columns=[\"transaction_id\"]).to_parquet(\"temp-train.parquet\")\n",
    "train_indexes = spark.read.parquet(\"temp-train.parquet\").repartition(1).persist(StorageLevel.DISK_ONLY)\n",
    "print(train_indexes.count())\n",
    "pd.DataFrame(validation_indexes, columns=[\"transaction_id\"]).to_parquet(\"temp-valid.parquet\")\n",
    "validation_indexes = spark.read.parquet(\"temp-valid.parquet\").repartition(1).persist(StorageLevel.DISK_ONLY)\n",
    "print(validation_indexes.count())\n",
    "pd.DataFrame(test_indexes, columns=[\"transaction_id\"]).to_parquet(\"temp-test.parquet\")\n",
    "test_indexes = spark.read.parquet(\"temp-test.parquet\").repartition(1).persist(StorageLevel.DISK_ONLY)\n",
    "print(test_indexes.count())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d5837a4-1085-4d7e-82e9-c9f61afa24c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ")\n",
    "validation = validation_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ")\n",
    "test = test_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ")\n",
    "train_validation = train.union(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eb99d4c-0be6-4c79-bff1-4da1152d22f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_main_features = os.path.join(\"features\", s.OUTPUT_POSTFIX.lstrip(\"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dca4b9b1-73fa-446d-81ea-223a18127df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(location_main_features, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a29581a8-b855-403c-adda-c3c0aa717dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_train = f\"{location_main_features}{os.sep}train{os.sep}\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(location_main_features)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.makedirs(location_train)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88213f0e-2497-4f01-b735-f941102e4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# data_input = data.select(\"*\")\n",
    "# nodes_source = set(get_pandas(data.select(\"source\").distinct())[\"source\"])\n",
    "# nodes_target = set(get_pandas(data.select(\"target\").distinct())[\"target\"])\n",
    "# nodes_passthrough = nodes_source.intersection(nodes_target)\n",
    "\n",
    "# %run communities_global.ipynb\n",
    "\n",
    "# comm_as_source_features.to_parquet(f\"{location_main_features}/comm_as_source_features.parquet\")\n",
    "# comm_as_target_features.to_parquet(f\"{location_main_features}/comm_as_target_features.parquet\")\n",
    "# comm_as_passthrough_features.to_parquet(f\"{location_main_features}/comm_as_passthrough_features.parquet\")\n",
    "# comm_as_passthrough_features_reverse.to_parquet(f\"{location_main_features}/comm_as_passthrough_features_reverse.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6ed3b9b-551e-4b78-a338-f7c800de7eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_as_source_features = pd.read_parquet(f\"{location_main_features}/comm_as_source_features.parquet\")\n",
    "comm_as_target_features = pd.read_parquet(f\"{location_main_features}/comm_as_target_features.parquet\")\n",
    "comm_as_passthrough_features = pd.read_parquet(f\"{location_main_features}/comm_as_passthrough_features.parquet\")\n",
    "comm_as_passthrough_features_reverse = pd.read_parquet(f\"{location_main_features}/comm_as_passthrough_features_reverse.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48961885-70d5-4fd6-bf88-55694f326092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.97 s, sys: 913 ms, total: 4.88 s\n",
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ts_min = data_graph.select(sf.min(\"timestamp\").alias(\"x\")).collect()[0][\"x\"] - timedelta(minutes=1)\n",
    "data_graph_agg = data_graph.groupby([\"source\", \"target\", \"source_bank\", \"target_bank\", \"source_currency\"]).agg(\n",
    "    sf.count(\"source\").alias(\"num_transactions\"),\n",
    "    sf.sum(\"amount\").alias(\"amount\"),\n",
    "    sf.sum(\"source_amount\").alias(\"source_amount\"),\n",
    "    sf.to_json(\n",
    "        sf.collect_list(sf.array((sf.col(\"timestamp\") - ts_min).cast(\"long\"), sf.col(\"amount\")))\n",
    "    ).alias(\"timestamps_amounts\"),\n",
    ")\n",
    "data_graph_agg = get_pandas(data_graph_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f45f4cf-663d-4fb9-8b90-4064bd2cf3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing features\n",
      "CPU times: user 14 s, sys: 8.82 s, total: 22.8 s\n",
      "Wall time: 1h 16min 46s\n",
      "CPU times: user 5min 41s, sys: 32.1 s, total: 6min 13s\n",
      "Wall time: 2h 34min 47s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# %run model_experiment_nested_new.ipynb\n",
    "\n",
    "# all_features = all_features.join(\n",
    "#     comm_as_source_features, how=\"left\", rsuffix=\"_dispense\"\n",
    "# ).join(\n",
    "#     comm_as_target_features, how=\"left\", rsuffix=\"_sink\"\n",
    "# ).join(\n",
    "#     comm_as_passthrough_features, how=\"left\", rsuffix=\"_passthrough\"\n",
    "# ).join(\n",
    "#     comm_as_passthrough_features_reverse, how=\"left\", rsuffix=\"_passthrough_rev\"\n",
    "# )\n",
    "\n",
    "# all_features.to_parquet(f\"{location_main_features}/features_if.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a41180a-7206-47ee-b292-9b2d7fbc8e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.read_parquet(f\"{location_main_features}/features_if.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b7526b8-de67-4aa9-93b5-352a5bb67d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "anomalies = all_features.loc[:, []]\n",
    "anomalies.loc[:, \"anomaly_score\"] = IsolationForest().fit(\n",
    "    all_features.fillna(0)\n",
    ").decision_function(all_features.fillna(0))\n",
    "anomalies.loc[:, \"anomaly_score\"] += abs(anomalies.loc[:, \"anomaly_score\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c04f2ba-88f3-49d0-b6d8-d6266d19dfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998574501186977\n",
      "CPU times: user 27.9 s, sys: 1.49 s, total: 29.4 s\n",
      "Wall time: 4.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "all_features_dim_reduced = pd.DataFrame(\n",
    "    pca.fit_transform(normalize(all_features.fillna(0), norm=\"l1\", axis=1)),\n",
    "    index=all_features.index\n",
    ")\n",
    "print(sum(pca.explained_variance_ratio_))\n",
    "all_features_dim_reduced.columns = [\n",
    "    f\"pca_{x + 1}\" for x in all_features_dim_reduced.columns\n",
    "]\n",
    "# del all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747aae1c-6460-4455-b46a-4fe2dec98044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "edge_features = []\n",
    "for index, (k, v) in enumerate(\n",
    "    get_pandas(\n",
    "        data.select(\"source\", \"target\", \"source_currency\", \"source_amount\", \"amount\", \"format\")\n",
    "    ).groupby([\"source\", \"target\"])\n",
    "):\n",
    "    if not (index % 100_000):\n",
    "        print(index)\n",
    "    edge_features.append(get_edge_features(k, v))\n",
    "edge_features = pd.DataFrame(edge_features)\n",
    "edge_features.to_parquet(f\"{location_main_features}/features_edges.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456cecb-97a0-45c6-bffc-a9db2d004ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_features = pd.read_parquet(f\"{location_main_features}/features_edges.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2884d7dd-1069-4275-b582-66e3604f45db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_edges = get_pandas(train.select(\"source\", \"target\").drop_duplicates()).set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "valid_edges = get_pandas(validation.select(\"source\", \"target\").drop_duplicates()).set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "test_edges = get_pandas(test.select(\"source\", \"target\").drop_duplicates()).set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a603e-a2af-41bc-8c10-f71c4bae2e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_edges.join(\n",
    "    edge_features.set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "validation_features = valid_edges.join(\n",
    "    edge_features.set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "test_features = test_edges.join(\n",
    "    edge_features.set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf80de-4720-4c0a-8d5b-3bfafd0a26c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.set_index(\"target\").join(\n",
    "    anomalies, how=\"left\"\n",
    ").reset_index().set_index(\"source\").join(\n",
    "    anomalies, how=\"left\", rsuffix=\"_source\"\n",
    ").reset_index().set_index(\"target\").join(\n",
    "    all_features_dim_reduced, how=\"left\"\n",
    ").reset_index().set_index(\"source\").join(\n",
    "    all_features_dim_reduced, how=\"left\", rsuffix=\"_source\"\n",
    ").reset_index()\n",
    "train_features.loc[:, \"anom_scores_diff\"] = train_features.loc[:, \"anomaly_score\"] - train_features.loc[:, \"anomaly_score_source\"]\n",
    "train_features.loc[:, \"anom_scores_min\"] = np.array(\n",
    "    [\n",
    "        train_features.loc[:, \"anomaly_score\"].values, \n",
    "        train_features.loc[:, \"anomaly_score_source\"].values\n",
    "    ],\n",
    ").min(axis=0)\n",
    "train_features.loc[:, \"anom_scores_max\"] = np.array(\n",
    "    [\n",
    "        train_features.loc[:, \"anomaly_score\"].values, \n",
    "        train_features.loc[:, \"anomaly_score_source\"].values\n",
    "    ],\n",
    ").max(axis=0)\n",
    "train_features.loc[:, \"anom_scores_mean\"] = np.array(\n",
    "    [\n",
    "        train_features.loc[:, \"anomaly_score\"].values, \n",
    "        train_features.loc[:, \"anomaly_score_source\"].values\n",
    "    ],\n",
    ").mean(axis=0)\n",
    "train_features.to_parquet(f\"{location_train}/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbd114-df75-4bca-b697-7fcd08ae36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = validation_features.set_index(\"target\").join(\n",
    "    anomalies, how=\"left\"\n",
    ").reset_index().set_index(\"source\").join(\n",
    "    anomalies, how=\"left\", rsuffix=\"_source\"\n",
    ").reset_index().set_index(\"target\").join(\n",
    "    all_features_dim_reduced, how=\"left\"\n",
    ").reset_index().set_index(\"source\").join(\n",
    "    all_features_dim_reduced, how=\"left\", rsuffix=\"_source\"\n",
    ").reset_index()\n",
    "validation_features.loc[:, \"anom_scores_diff\"] = validation_features.loc[:, \"anomaly_score\"] - validation_features.loc[:, \"anomaly_score_source\"]\n",
    "validation_features.loc[:, \"anom_scores_min\"] = np.array(\n",
    "    [\n",
    "        validation_features.loc[:, \"anomaly_score\"].values, \n",
    "        validation_features.loc[:, \"anomaly_score_source\"].values\n",
    "    ],\n",
    ").min(axis=0)\n",
    "validation_features.loc[:, \"anom_scores_max\"] = np.array(\n",
    "    [\n",
    "        validation_features.loc[:, \"anomaly_score\"].values, \n",
    "        validation_features.loc[:, \"anomaly_score_source\"].values\n",
    "    ],\n",
    ").max(axis=0)\n",
    "validation_features.loc[:, \"anom_scores_mean\"] = np.array(\n",
    "    [\n",
    "        validation_features.loc[:, \"anomaly_score\"].values, \n",
    "        validation_features.loc[:, \"anomaly_score_source\"].values\n",
    "    ],\n",
    ").mean(axis=0)\n",
    "validation_features.to_parquet(f\"{location_train}/validation.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d4fed8-cc76-44e0-809a-5d6ae82539c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test_features.set_index(\"target\").join(\n",
    "    anomalies, how=\"left\"\n",
    ").reset_index().set_index(\"source\").join(\n",
    "    anomalies, how=\"left\", rsuffix=\"_source\"\n",
    ").reset_index().set_index(\"target\").join(\n",
    "    all_features_dim_reduced, how=\"left\"\n",
    ").reset_index().set_index(\"source\").join(\n",
    "    all_features_dim_reduced, how=\"left\", rsuffix=\"_source\"\n",
    ").reset_index()\n",
    "test_features.loc[:, \"anom_scores_diff\"] = test_features.loc[:, \"anomaly_score\"] - test_features.loc[:, \"anomaly_score_source\"]\n",
    "test_features.loc[:, \"anom_scores_min\"] = np.array(\n",
    "    [\n",
    "        test_features.loc[:, \"anomaly_score\"].values, \n",
    "        test_features.loc[:, \"anomaly_score_source\"].values\n",
    "    ],\n",
    ").min(axis=0)\n",
    "test_features.loc[:, \"anom_scores_max\"] = np.array(\n",
    "    [\n",
    "        test_features.loc[:, \"anomaly_score\"].values, \n",
    "        test_features.loc[:, \"anomaly_score_source\"].values\n",
    "    ],\n",
    ").max(axis=0)\n",
    "test_features.loc[:, \"anom_scores_mean\"] = np.array(\n",
    "    [\n",
    "        test_features.loc[:, \"anomaly_score\"].values, \n",
    "        test_features.loc[:, \"anomaly_score_source\"].values\n",
    "    ],\n",
    ").mean(axis=0)\n",
    "test_features.to_parquet(f\"{location_train}/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783f612-4c76-4ea5-aed2-68eeaec1191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"source\", \"target\", \"source_currency\", \"target_currency\", \"format\", \"amount\", \"is_laundering\"]\n",
    "columns_category = [\"source_currency\", \"target_currency\", \"format\"]\n",
    "train_trx_features = get_pandas(train.select(*columns))\n",
    "train_trx_features.loc[:, \"inter_currency\"] = train_trx_features[\"source_currency\"] != train_trx_features[\"target_currency\"]\n",
    "valid_trx_features = get_pandas(validation.select(*columns))\n",
    "valid_trx_features.loc[:, \"inter_currency\"] = valid_trx_features[\"source_currency\"] != valid_trx_features[\"target_currency\"]\n",
    "test_trx_features = get_pandas(test.select(*columns))\n",
    "test_trx_features.loc[:, \"inter_currency\"] = test_trx_features[\"source_currency\"] != test_trx_features[\"target_currency\"]\n",
    "\n",
    "new_types = {column: \"category\" for column in columns_category}\n",
    "train_trx_features = train_trx_features.astypes(new_types)\n",
    "valid_trx_features = valid_trx_features.astypes(new_types)\n",
    "test_trx_features = test_trx_features.astypes(new_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea56f4-4dba-4cb4-a95e-b8989ca7b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "label_columns = [\"source\", \"target\", \"is_laundering\"]\n",
    "\n",
    "train_features = pd.read_parquet(f\"{location_train}/train.parquet\")\n",
    "del train_features[\"is_laundering\"]\n",
    "validation_features = pd.read_parquet(f\"{location_train}/validation.parquet\")\n",
    "del validation_features[\"is_laundering\"]\n",
    "test_features = pd.read_parquet(f\"{location_train}/test.parquet\")\n",
    "del test_features[\"is_laundering\"]\n",
    "\n",
    "train_features = train_trx_features.set_index([\"source\", \"target\"]).join(\n",
    "    train_features.set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "# TODO: Why is `format_Reinvestment` not in validation/test data\n",
    "del train_features[\"format_Reinvestment\"]\n",
    "\n",
    "validation_features = valid_trx_features.set_index([\"source\", \"target\"]).join(\n",
    "    validation_features.set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "\n",
    "test_features = test_trx_features.set_index([\"source\", \"target\"]).join(\n",
    "    test_features.set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "\n",
    "train_features_labels = train_features.loc[:, label_columns].copy(deep=True)\n",
    "del train_features[\"is_laundering\"]\n",
    "del train_features[\"source\"]\n",
    "del train_features[\"target\"]\n",
    "\n",
    "validation_features_labels = validation_features.loc[:, label_columns].copy(deep=True)\n",
    "validation_features = validation_features.loc[:, train_features.columns]\n",
    "\n",
    "test_features_labels = test_features.loc[:, label_columns].copy(deep=True)\n",
    "test_features = test_features.loc[:, train_features.columns]\n",
    "# test_labels_orig = test.select([\"source\", \"target\", \"is_laundering\"]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafdcaf7-7031-4259-b972-f946ba09cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_eval(y, y_):\n",
    "    return 1 - f1_score(y, np.round(y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2dbe51-59bf-4e8e-a132-c4c3121ae717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For HI\n",
    "# def train_model(x, y, x_, y_):\n",
    "#     model = xgb.XGBClassifier(\n",
    "#         early_stopping_rounds=20, scale_pos_weight=10,\n",
    "#         eval_metric=f1_eval, disable_default_eval_metric=True, num_parallel_tree=20, max_depth=6,\n",
    "#         colsample_bytree=0.5, subsample=0.5, enable_categorical=True,\n",
    "#     )\n",
    "#     model.fit(x, y, verbose=False, eval_set=[(x_, y_)])\n",
    "#     print(f\"Best iteration: {model.best_iteration}\\n\")\n",
    "#     return model\n",
    "\n",
    "\n",
    "# For LI\n",
    "def train_model(x, y, x_, y_):\n",
    "    # model = xgb.XGBClassifier(\n",
    "    #     scale_pos_weight=10,\n",
    "    #     eval_metric=f1_eval, disable_default_eval_metric=True, num_parallel_tree=20, max_depth=6,\n",
    "    #     colsample_bytree=0.5, subsample=0.5, n_estimators=1000,\n",
    "    #     enable_categorical=True,\n",
    "    # )\n",
    "    model = xgb.XGBClassifier(\n",
    "        early_stopping_rounds=20, scale_pos_weight=5,\n",
    "        eval_metric=f1_eval, disable_default_eval_metric=True, num_parallel_tree=5, max_depth=6,\n",
    "        colsample_bytree=0.5, subsample=0.5, n_estimators=500,\n",
    "        enable_categorical=True,\n",
    "    )\n",
    "    model.fit(x, y, verbose=False, eval_set=[(x_, y_)])\n",
    "    print(f\"Best iteration: {model.best_iteration}\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee1446-ca93-4830-b979-0852f74eaacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orig_prediction_data(labels_data, labels_orig, prediction_values):\n",
    "    labels_data = labels_data.copy(deep=True)\n",
    "    labels_orig = labels_orig.copy(deep=True)\n",
    "    labels_data.loc[:, \"predicted\"] = prediction_values\n",
    "    predictions_agg = labels_data.groupby([\"source\", \"target\"]).agg(\n",
    "        predicted=(\"predicted\", \"max\")\n",
    "    ).reset_index()\n",
    "    final_predictions = labels_orig.set_index([\"source\", \"target\"]).join(\n",
    "        predictions_agg.set_index([\"source\", \"target\"]), how=\"left\"\n",
    "    ).reset_index()\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1642f1-ce3f-4a20-8b99-f0362047c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = train_model(\n",
    "    train_features, train_features_labels[\"is_laundering\"].values, \n",
    "    validation_features, validation_features_labels[\"is_laundering\"].values\n",
    ")\n",
    "y_test_predicted = model.predict(test_features)\n",
    "print(\n",
    "    \"aggregated\",\n",
    "    round(f1_score(test_features_labels[\"is_laundering\"], y_test_predicted) * 100, 2),\n",
    "    round(recall_score(test_features_labels[\"is_laundering\"], y_test_predicted) * 100, 2)\n",
    ")\n",
    "# predictions_data = get_orig_prediction_data(\n",
    "#     test_features_labels, test_labels_orig, y_test_predicted\n",
    "# )\n",
    "# f1_final = round(f1_score(predictions_data[\"is_laundering\"], predictions_data[\"predicted\"]) * 100, 2)\n",
    "# print(\n",
    "#     \"final\",\n",
    "#     f1_final,\n",
    "#     round(recall_score(predictions_data[\"is_laundering\"], predictions_data[\"predicted\"]) * 100, 2)\n",
    "# )\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e178c2-8cf6-4edb-ae31-320420efc8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "CV_FOLD_PERC = 0.8\n",
    "N_FOLDS = 5\n",
    "\n",
    "f1_scores = []\n",
    "for fold in range(N_FOLDS):\n",
    "    print(\"Fold\", fold + 1)\n",
    "    x_train = train_features.sample(frac=CV_FOLD_PERC)\n",
    "    x_train_labels = x_train.loc[:, []].join(train_features_labels, how=\"left\")\n",
    "    x_validation = validation_features.sample(frac=CV_FOLD_PERC)\n",
    "    x_validation_labels = x_validation.loc[:, []].join(validation_features_labels, how=\"left\")\n",
    "    model = train_model(\n",
    "        x_train, x_train_labels[\"is_laundering\"].values, \n",
    "        x_validation, x_validation_labels[\"is_laundering\"].values\n",
    "    )\n",
    "    y_test_predicted = model.predict(test_features)\n",
    "    predictions_data = get_orig_prediction_data(\n",
    "        test_features_labels, test_labels_orig, y_test_predicted\n",
    "    )\n",
    "    f1_cv = f1_score(predictions_data[\"is_laundering\"], predictions_data[\"predicted\"]) * 100\n",
    "    print(\n",
    "        round(f1_cv, 2),\n",
    "        round(recall_score(predictions_data[\"is_laundering\"], predictions_data[\"predicted\"]) * 100, 2)\n",
    "    )\n",
    "    f1_scores.append(f1_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6bbb2c-8383-43e7-91f2-447c9b013683",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{f1_final} Â±{round(np.std(f1_scores), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c5a8dc-1b84-4e39-bf01-27b6f39959a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((time.time() - start) // 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2b7dc-3e04-4d18-b234-e928bb6b0f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
