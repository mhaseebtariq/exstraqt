{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6844f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from glob import glob\n",
    "from datetime import timedelta, datetime\n",
    "from itertools import combinations\n",
    "\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "\n",
    "import settings as s\n",
    "from common import get_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c7f1e2-c0f3-4464-b223-ddd8c6fef69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/06 14:54:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "config = [\n",
    "    (\"spark.driver.memory\", \"16g\"),\n",
    "    (\"spark.worker.memory\", \"16g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"16g\"),\n",
    "]\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(config))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e69899e-e6a0-4f84-a288-737de0dd3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fdc571c-b31c-4970-8f32-a9955cceb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 14\n",
    "TRAIN_PERC = 0.6\n",
    "VALIDATION_PERC = 0.2\n",
    "TEST_PERC = 0.2\n",
    "\n",
    "assert(sum([TRAIN_PERC, VALIDATION_PERC, TEST_PERC]) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c35a497-3f7f-498a-8a5a-da1a78d1f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet(s.STAGED_DATA_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2103dcc9-a709-4d42-b17a-9f403aa684ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-10 23:59:00\n"
     ]
    }
   ],
   "source": [
    "# The last few days only contain incomplete data\n",
    "trx_count_per_day = data.groupby(sf.to_date(\"timestamp\").alias(\"date\")).count().toPandas()\n",
    "trx_count_per_day = trx_count_per_day.sort_values(\"date\").set_index(\"date\")\n",
    "mean_per_day = np.mean(trx_count_per_day[\"count\"])\n",
    "mean_per_day_ratio = trx_count_per_day[\"count\"] / mean_per_day\n",
    "complete_data_present_till = max(mean_per_day_ratio[mean_per_day_ratio > 0.1].index)\n",
    "complete_data_present_till = data.where(sf.to_date(\"timestamp\") == complete_data_present_till).select(\n",
    "    sf.max(\"timestamp\").alias(\"x\")\n",
    ").collect()[0][\"x\"]\n",
    "print(complete_data_present_till)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b936c12-027a-4cec-8360-76fb92def8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "trx_ids_sorted = data.sort(\"timestamp\").select(\"transaction_id\").toPandas()[\"transaction_id\"].values\n",
    "trx_count = len(trx_ids_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a13395-4873-4612-b464-1e8103b5ff75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/06 14:55:01 WARN TaskSetManager: Stage 11 contains a task of very large size (1645 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3043615\n",
      "1014538\n",
      "1014540\n"
     ]
    }
   ],
   "source": [
    "last_train_index = int(np.floor(trx_count * TRAIN_PERC))\n",
    "last_validation_index = last_train_index + int(np.floor(trx_count * VALIDATION_PERC))\n",
    "train_indexes = trx_ids_sorted[:last_train_index]\n",
    "validation_indexes = trx_ids_sorted[last_train_index:last_validation_index]\n",
    "test_indexes = trx_ids_sorted[last_validation_index:]\n",
    "\n",
    "train_indexes = spark.createDataFrame(\n",
    "    pd.DataFrame(train_indexes, columns=[\"transaction_id\"])\n",
    ").repartition(1).cache()\n",
    "print(train_indexes.count())\n",
    "validation_indexes = spark.createDataFrame(\n",
    "    pd.DataFrame(validation_indexes, columns=[\"transaction_id\"])\n",
    ").repartition(1).cache()\n",
    "print(validation_indexes.count())\n",
    "test_indexes = spark.createDataFrame(\n",
    "    pd.DataFrame(test_indexes, columns=[\"transaction_id\"])\n",
    ").repartition(1).cache()\n",
    "print(test_indexes.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d5837a4-1085-4d7e-82e9-c9f61afa24c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ")\n",
    "validation = validation_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ")\n",
    "test = test_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ")\n",
    "train_validation = train.union(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a31321-bf63-4ed6-9323-171b71a161b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pandas(df):\n",
    "    df.write.parquet(\"temp.parquet\", mode=\"overwrite\")\n",
    "    df = pd.read_parquet(\"temp.parquet\")\n",
    "    # Because of tz discrepancy\n",
    "    df.loc[:, \"timestamp\"] += timedelta(hours=2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b9aa236-047f-41c9-b7c3-a78b5a3a02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windowed_datasets(data_dates, data_input):\n",
    "    for date_trx in sorted([x[\"timestamp\"] for x in data_dates.select(\"timestamp\").distinct().collect()]):\n",
    "        datetime_trx_start = datetime.combine(date_trx, datetime.min.time())\n",
    "        datetime_trx_end = datetime.combine(date_trx, datetime.max.time())\n",
    "        left_start = datetime_trx_start - timedelta(WINDOW_SIZE)\n",
    "        right_end = datetime_trx_end + timedelta(WINDOW_SIZE)\n",
    "        left = get_pandas(\n",
    "            data_input.where(\n",
    "                (data_input[\"timestamp\"] >= left_start) & (data_input[\"timestamp\"] <= datetime_trx_end)\n",
    "            )\n",
    "        )\n",
    "        right = get_pandas(\n",
    "            data_input.where(\n",
    "                (data_input[\"timestamp\"] >= datetime_trx_start) & (data_input[\"timestamp\"] <= right_end)\n",
    "            )\n",
    "        )\n",
    "        pov = get_pandas(\n",
    "            data_input.where(\n",
    "                (data_input[\"timestamp\"] >= datetime_trx_start) & (data_input[\"timestamp\"] <= datetime_trx_end)\n",
    "            )\n",
    "        )\n",
    "        yield(left, pov, right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f45f4cf-663d-4fb9-8b90-4064bd2cf3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 781 ms, sys: 115 ms, total: 896 ms\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for left_df, pov_df, right_df in get_windowed_datasets(train, train):\n",
    "    for in_scope_window in [left_df, right_df]:\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5620d41b-5eff-460a-b1ec-1d5a03143ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_scope_window.loc[:, \"window_delta\"] = (in_scope_window[\"timestamp\"] - in_scope_window[\"timestamp\"].min()).dt.seconds + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48851c02-176f-40d0-bb7f-5a517cbcb107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of data quality issue\n",
    "\n",
    "in_scope_window_size = (\n",
    "    min([in_scope_window[\"timestamp\"].max(), complete_data_present_till]) -\n",
    "    in_scope_window[\"timestamp\"].min()\n",
    ").seconds\n",
    "if in_scope_window_size < 1:\n",
    "    in_scope_window_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9097a31-0809-4c06-a2bd-07610f84e050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86340"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_scope_window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efbf9d8f-8b05-47df-8988-466103096474",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_scope_edges = pov_df.groupby([\"source\", \"target\"]).agg(is_laundering=(\"is_laundering\", \"max\")).reset_index()\n",
    "in_scope_nodes = set(in_scope_edges[\"source\"]).union(in_scope_edges[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "025a70e3-bf93-4424-9a61-0f34d7dadf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = in_scope_window.groupby([\"source\", \"target\"]).agg(\n",
    "    amount=(\"amount\", \"sum\"),\n",
    "    is_laundering=(\"is_laundering\", \"max\"),\n",
    ").reset_index()\n",
    "nodes.loc[:, \"id\"] = [f\"i-{x}\" for x in nodes.index]\n",
    "nodes = nodes.loc[:, [\"id\", \"source\", \"target\", \"amount\", \"is_laundering\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "143e4c3c-4790-4a42-913f-f82b41cc267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_fo = ig.Graph.DataFrame(\n",
    "    nodes.loc[:, [\"source\", \"target\", \"amount\"]].rename(columns={\"amount\": \"weight\"}), \n",
    "    use_vids=False, directed=True\n",
    ")\n",
    "nodes_fo = list(in_scope_nodes.intersection([x[\"name\"] for x in graph_fo.vs()]))\n",
    "random.shuffle(nodes_fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7590157-b927-41ee-8918-cf7efaa31646",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = nodes.set_index(\"target\")\n",
    "right = nodes.set_index(\"source\")\n",
    "edges = left.join(right, how=\"inner\", lsuffix=\"_left\")\n",
    "edges = edges.loc[edges[\"id_left\"] != edges[\"id\"], :].reset_index(drop=True)\n",
    "edges = edges[[\"id_left\", \"id\", \"amount_left\", \"amount\"]].rename(\n",
    "    columns={\"id_left\": \"source\", \"id\": \"target\"}\n",
    ")\n",
    "edges.loc[:, \"weight\"] = 1 - abs(edges[\"amount_left\"] - edges[\"amount\"]) / (\n",
    "    edges[\"amount_left\"] + edges[\"amount\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b53a8972-c753-4121-a1ee-f110bad99cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_so = ig.Graph.DataFrame(edges, use_vids=False, directed=True)\n",
    "nodes_so = set(\n",
    "    [x[\"name\"] for x in graph_so.vs()]\n",
    ").intersection(\n",
    "    nodes[[\"id\", \"source\", \"target\"]].set_index([\"source\", \"target\"]).join(\n",
    "        in_scope_edges[[\"source\", \"target\"]].set_index([\"source\", \"target\"]), how=\"inner\"\n",
    "    )[\"id\"].tolist()\n",
    ")\n",
    "nodes_so = list(nodes_so)\n",
    "random.shuffle(nodes_so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "507b1c14-89a6-47c2-bcb2-93d5b8083066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done -> 0\n",
      "Done -> 6\n",
      "Done -> 3\n",
      "Done -> 7\n",
      "Done -> 8\n",
      "Done -> 1\n",
      "Done -> 2\n",
      "Done -> 9\n",
      "Done -> 4\n",
      "Done -> 5\n",
      "CPU times: user 292 ms, sys: 234 ms, total: 525 ms\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "NUMBER_OF_PROCESSES = 10\n",
    "\n",
    "shutil.rmtree(\"staging\", ignore_errors=True)\n",
    "os.mkdir(\"staging\")\n",
    "chunks = np.array_split(nodes_fo, NUMBER_OF_PROCESSES)\n",
    "\n",
    "filename = \"graph.pickle\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(graph_fo, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "filename = \"nodes.pickle\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(chunks, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "process_ids = set()\n",
    "process_name = \"communities.py\"\n",
    "for chunk_number in range(NUMBER_OF_PROCESSES):\n",
    "    process_id = str(uuid.uuid4())\n",
    "    process_ids = process_ids.union({process_id})\n",
    "    os.system(f\"{sys.executable} {process_name} {chunk_number} {process_id} &\")\n",
    "\n",
    "while get_processes(process_ids):\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce94e8c6-3347-4b68-99b3-9e7802ea36df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.7 s, sys: 79.7 ms, total: 1.78 s\n",
      "Wall time: 1.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 35, 1121276)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for proc in get_processes(process_ids):\n",
    "    try:\n",
    "        proc.kill()\n",
    "    except psutil.NoSuchProcess:\n",
    "        pass\n",
    "\n",
    "communities_fo = []\n",
    "for filename in glob(\"./staging/*.pickle\"):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        communities_fo += pickle.load(f)\n",
    "\n",
    "original_size = len(communities_fo)\n",
    "\n",
    "filename = \"communities.pickle\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(communities_fo, f)\n",
    "\n",
    "sizes = [len(x[1]) for x in communities_fo]\n",
    "round(np.mean(sizes)), round(np.max(sizes)), sum(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "237b669e-2c44-49cb-b72b-a86f1584d63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done -> 2\n",
      "Done -> 5\n",
      "Done -> 3\n",
      "Done -> 1\n",
      "Done -> 4\n",
      "Done -> 8\n",
      "Done -> 0\n",
      "Done -> 6\n",
      "Done -> 9\n",
      "Done -> 7\n",
      "CPU times: user 410 ms, sys: 263 ms, total: 673 ms\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "NUMBER_OF_PROCESSES = 10\n",
    "\n",
    "shutil.rmtree(\"staging\", ignore_errors=True)\n",
    "os.mkdir(\"staging\")\n",
    "chunks = np.array_split(nodes_so, NUMBER_OF_PROCESSES)\n",
    "\n",
    "filename = \"graph.pickle\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(graph_so, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "filename = \"nodes.pickle\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(chunks, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "process_ids = set()\n",
    "process_name = \"communities.py\"\n",
    "for chunk_number in range(NUMBER_OF_PROCESSES):\n",
    "    process_id = str(uuid.uuid4())\n",
    "    process_ids = process_ids.union({process_id})\n",
    "    os.system(f\"{sys.executable} {process_name} {chunk_number} {process_id} &\")\n",
    "\n",
    "while get_processes(process_ids):\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3eda56f5-670a-43a4-ad7b-44c8cb261f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.45 s, sys: 130 ms, total: 3.58 s\n",
      "Wall time: 3.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6, 42, 3267954)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for proc in get_processes(process_ids):\n",
    "    try:\n",
    "        proc.kill()\n",
    "    except psutil.NoSuchProcess:\n",
    "        pass\n",
    "\n",
    "communities_so = []\n",
    "for filename in glob(\"./staging/*.pickle\"):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        communities_so += pickle.load(f)\n",
    "\n",
    "original_size = len(communities_so)\n",
    "\n",
    "filename = \"communities.pickle\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(communities_so, f)\n",
    "\n",
    "sizes = [len(x[1]) for x in communities_so]\n",
    "round(np.mean(sizes)), round(np.max(sizes)), sum(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d639f2f8-7c79-4b8e-9125-a527b0523bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_scope_window.loc[:, \"edge\"] = in_scope_window.apply(\n",
    "    lambda x: tuple(sorted([x[\"source\"], x[\"target\"]])), axis=1\n",
    ")\n",
    "in_scope_window.loc[:, \"edge\"] = in_scope_window.loc[:, \"edge\"].apply(\n",
    "    lambda x: f\"{x[0]}-{x[1]}\"\n",
    ")\n",
    "in_scope_window.set_index(\"edge\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0f6f957-bffe-4080-a5fd-702cacabd357",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_trx_comm_fo = \"transactions_communities_fo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e192cc35-75b4-489e-940c-113ab99ebf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9\n",
      "5 9\n",
      "CPU times: user 6.52 s, sys: 124 ms, total: 6.64 s\n",
      "Wall time: 6.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "shutil.rmtree(location_trx_comm_fo, ignore_errors=True)\n",
    "os.mkdir(location_trx_comm_fo)\n",
    "\n",
    "shutil.rmtree(location_trx_comm_fo, ignore_errors=True)\n",
    "os.mkdir(location_trx_comm_fo)\n",
    "\n",
    "communities_fo = dict(communities_fo)\n",
    "communities_keys = [x for x in communities_fo.keys()]\n",
    "\n",
    "number_of_chunks = int(np.ceil(len(communities_keys) / 50_000))\n",
    "chunks = np.array_split(communities_keys, number_of_chunks)\n",
    "for index, chunk in enumerate(chunks):\n",
    "    comm_inner = []\n",
    "    for key in chunk:\n",
    "        comm_node = communities_fo[key]\n",
    "        comm_inner += [[key, tuple(sorted(x))] for x in combinations(comm_node, 2)]\n",
    "    edge_combinations = pd.DataFrame(comm_inner, columns=[\"id\", \"edge\"])\n",
    "    edge_combinations.loc[:, \"edge\"] = edge_combinations.loc[:, \"edge\"].apply(\n",
    "        lambda x: f\"{x[0]}-{x[1]}\"\n",
    "    )\n",
    "    edge_combinations.set_index(\"edge\", inplace=True)\n",
    "    edge_combinations.join(in_scope_window, how=\"inner\").reset_index(drop=True).to_parquet(\n",
    "        f\"{location_trx_comm_fo}/part-{index}.parquet\"\n",
    "    )\n",
    "    if not (index % 5):\n",
    "        print(index, len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d61f8786-cca3-4605-b605-e9cdd577b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trx_communities_fo = pd.read_parquet(location_trx_comm_fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9364117-335a-41ef-8dd0-4241a4edd2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.loc[:, \"source_target\"] = nodes.apply(lambda x: (x[\"source\"], x[\"target\"]), axis=1)\n",
    "edges_mapping = nodes.set_index(\"id\")[\"source_target\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "029a92f2-e696-4402-a082-b20e88fabe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_trx_comm_so = \"transactions_communities_so\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94503ceb-5562-4814-8eb1-033f2ee42940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11\n",
      "5 11\n",
      "10 11\n",
      "CPU times: user 1min 49s, sys: 5.53 s, total: 1min 54s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "shutil.rmtree(location_trx_comm_so, ignore_errors=True)\n",
    "os.mkdir(location_trx_comm_so)\n",
    "\n",
    "shutil.rmtree(location_trx_comm_so, ignore_errors=True)\n",
    "os.mkdir(location_trx_comm_so)\n",
    "\n",
    "communities_so = dict(communities_so)\n",
    "communities_so = {\n",
    "    edges_mapping[k]: [x for y in [edges_mapping[_] for _ in v] for x in y]\n",
    "    for k, v in communities_so.items()\n",
    "}\n",
    "communities_keys = [x for x in communities_so.keys()]\n",
    "\n",
    "number_of_chunks = int(np.ceil(len(communities_keys) / 50_000))\n",
    "chunks = np.array_split(communities_keys, number_of_chunks)\n",
    "for index, chunk in enumerate(chunks):\n",
    "    comm_inner = []\n",
    "    for key in chunk:\n",
    "        key = tuple(key)\n",
    "        comm_node = communities_so[key]\n",
    "        comm_inner += [[key, tuple(sorted(x))] for x in combinations(comm_node, 2)]\n",
    "    edge_combinations = pd.DataFrame(comm_inner, columns=[\"id\", \"edge\"])\n",
    "    edge_combinations.loc[:, \"edge\"] = edge_combinations.loc[:, \"edge\"].apply(\n",
    "        lambda x: f\"{x[0]}-{x[1]}\"\n",
    "    )\n",
    "    del edge_combinations[\"id\"]\n",
    "    edge_combinations.set_index(\"edge\", inplace=True)\n",
    "    edge_combinations.loc[:, \"id\"] = edge_combinations.index\n",
    "    edge_combinations.join(in_scope_window, how=\"inner\").to_parquet(\n",
    "        f\"{location_trx_comm_so}/part-{index}.parquet\"\n",
    "    )\n",
    "    if not (index % 5):\n",
    "        print(index, len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a106671-6412-4f97-be5b-f33a6b92a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trx_communities_so = pd.read_parquet(location_trx_comm_so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec7a2576-581a-4c23-9b51-f90aa4ab6133",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_features_fo = \"features_fo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92b4e09f-f530-4aa0-b1cd-cec4918edcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 171 ms, sys: 670 ms, total: 842 ms\n",
      "Wall time: 3min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "NUMBER_OF_PROCESSES = 10\n",
    "\n",
    "parts = sorted(\n",
    "    [x for x in glob(f\"{location_trx_comm_fo}/*.parquet\")],\n",
    "    key=lambda x: int(x.split(\"-\")[-1].split(\".\")[0]),\n",
    ")\n",
    "\n",
    "shutil.rmtree(location_features_fo, ignore_errors=True)\n",
    "os.mkdir(location_features_fo)\n",
    "\n",
    "process_ids = set()\n",
    "process_name = \"features.py\"\n",
    "while parts:\n",
    "    if len(get_processes(process_ids)) < NUMBER_OF_PROCESSES:\n",
    "        process_id = str(uuid.uuid4())\n",
    "        process_ids = process_ids.union({process_id})\n",
    "        os.system(\n",
    "            f\"{sys.executable} {process_name} {parts.pop()} {location_features_fo} {process_id} &\"\n",
    "        )\n",
    "\n",
    "while get_processes(process_ids):\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1eeda98c-463a-470a-900c-e5f3bcd04525",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_features_so = \"features_so\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea40262e-6e06-4b1b-ba08-a941353a641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NUMBER_OF_PROCESSES = 10\n",
    "\n",
    "parts = sorted(\n",
    "    [x for x in glob(f\"{location_trx_comm_so}/*.parquet\")],\n",
    "    key=lambda x: int(x.split(\"-\")[-1].split(\".\")[0]),\n",
    ")\n",
    "\n",
    "shutil.rmtree(location_features_so, ignore_errors=True)\n",
    "os.mkdir(location_features_so)\n",
    "\n",
    "process_ids = set()\n",
    "process_name = \"features.py\"\n",
    "while parts:\n",
    "    if len(get_processes(process_ids)) < NUMBER_OF_PROCESSES:\n",
    "        process_id = str(uuid.uuid4())\n",
    "        process_ids = process_ids.union({process_id})\n",
    "        os.system(\n",
    "            f\"{sys.executable} {process_name} {parts.pop()} {location_features_so} {process_id} &\"\n",
    "        )\n",
    "\n",
    "while get_processes(process_ids):\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380fbe7d-ec86-4645-ac8c-148f53998dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(\"features_so/\")[[\"ts_weighted_mean\", \"ts_weighted_median\", \"ts_weighted_std\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a071933-3aba-4a51-bd6d-6029899d41a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((time.time() - start) // 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
