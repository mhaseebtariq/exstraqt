{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6844f7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from glob import glob\n",
    "from datetime import timedelta, datetime\n",
    "from itertools import combinations\n",
    "\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "\n",
    "import settings as s\n",
    "from common import create_workload_for_multi_proc\n",
    "from communities import get_communities_multi_proc\n",
    "from features import get_features_multi_proc, get_pov_features\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c7f1e2-c0f3-4464-b223-ddd8c6fef69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/19 12:33:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "config = [\n",
    "    (\"spark.driver.memory\", \"16g\"),\n",
    "    (\"spark.worker.memory\", \"16g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"16g\"),\n",
    "]\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(config))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e69899e-e6a0-4f84-a288-737de0dd3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fdc571c-b31c-4970-8f32-a9955cceb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 7\n",
    "TRAIN_PERC = 0.6\n",
    "VALIDATION_PERC = 0.2\n",
    "TEST_PERC = 0.2\n",
    "\n",
    "NUM_PROCS = 10\n",
    "\n",
    "assert(sum([TRAIN_PERC, VALIDATION_PERC, TEST_PERC]) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c35a497-3f7f-498a-8a5a-da1a78d1f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet(s.STAGED_DATA_LOCATION)\n",
    "data = data.where(data[\"source\"] != data[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2103dcc9-a709-4d42-b17a-9f403aa684ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-10 23:59:00\n"
     ]
    }
   ],
   "source": [
    "# The last few days only contain incomplete data\n",
    "trx_count_per_day = data.groupby(sf.to_date(\"timestamp\").alias(\"date\")).count().toPandas()\n",
    "trx_count_per_day = trx_count_per_day.sort_values(\"date\").set_index(\"date\")\n",
    "mean_per_day = np.mean(trx_count_per_day[\"count\"])\n",
    "mean_per_day_ratio = trx_count_per_day[\"count\"] / mean_per_day\n",
    "complete_data_present_till = max(mean_per_day_ratio[mean_per_day_ratio > 0.1].index)\n",
    "complete_data_present_till = data.where(sf.to_date(\"timestamp\") == complete_data_present_till).select(\n",
    "    sf.max(\"timestamp\").alias(\"x\")\n",
    ").collect()[0][\"x\"]\n",
    "print(complete_data_present_till)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b936c12-027a-4cec-8360-76fb92def8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "trx_ids_sorted = data.sort(\"timestamp\").select(\"transaction_id\").toPandas()[\"transaction_id\"].values\n",
    "trx_count = len(trx_ids_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a13395-4873-4612-b464-1e8103b5ff75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/19 12:34:07 WARN TaskSetManager: Stage 11 contains a task of very large size (1507 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2689317\n",
      "896439\n",
      "896440\n"
     ]
    }
   ],
   "source": [
    "last_train_index = int(np.floor(trx_count * TRAIN_PERC))\n",
    "last_validation_index = last_train_index + int(np.floor(trx_count * VALIDATION_PERC))\n",
    "train_indexes = trx_ids_sorted[:last_train_index]\n",
    "validation_indexes = trx_ids_sorted[last_train_index:last_validation_index]\n",
    "test_indexes = trx_ids_sorted[last_validation_index:]\n",
    "\n",
    "train_indexes = spark.createDataFrame(\n",
    "    pd.DataFrame(train_indexes, columns=[\"transaction_id\"])\n",
    ").repartition(1).cache()\n",
    "print(train_indexes.count())\n",
    "validation_indexes = spark.createDataFrame(\n",
    "    pd.DataFrame(validation_indexes, columns=[\"transaction_id\"])\n",
    ").repartition(1).cache()\n",
    "print(validation_indexes.count())\n",
    "test_indexes = spark.createDataFrame(\n",
    "    pd.DataFrame(test_indexes, columns=[\"transaction_id\"])\n",
    ").repartition(1).cache()\n",
    "print(test_indexes.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d5837a4-1085-4d7e-82e9-c9f61afa24c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ")\n",
    "validation = validation_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ")\n",
    "test = test_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ")\n",
    "train_validation = train.union(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a31321-bf63-4ed6-9323-171b71a161b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pandas(df):\n",
    "    df.write.parquet(\"temp.parquet\", mode=\"overwrite\")\n",
    "    df = pd.read_parquet(\"temp.parquet\")\n",
    "    # Because of tz discrepancy\n",
    "    df.loc[:, \"timestamp\"] += timedelta(hours=2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b9aa236-047f-41c9-b7c3-a78b5a3a02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windowed_datasets(data_dates, data_input):\n",
    "    dates = data_dates.select(sf.to_date(\"timestamp\").alias(\"x\")).distinct().collect()\n",
    "    dates = sorted([x[\"x\"] for x in dates])\n",
    "    for date_trx in dates:\n",
    "        datetime_trx_start = datetime.combine(date_trx, datetime.min.time())\n",
    "        datetime_trx_end = datetime.combine(date_trx, datetime.max.time())\n",
    "        left_start = datetime_trx_start - timedelta(WINDOW_SIZE)\n",
    "        right_end = datetime_trx_end + timedelta(WINDOW_SIZE)\n",
    "        pov = get_pandas(\n",
    "            data_input.where(\n",
    "                (data_input[\"timestamp\"] >= datetime_trx_start) & (data_input[\"timestamp\"] <= datetime_trx_end)\n",
    "            )\n",
    "        )\n",
    "        window = get_pandas(\n",
    "            data_input.where(\n",
    "                (data_input[\"timestamp\"] >= left_start) & (data_input[\"timestamp\"] <= right_end)\n",
    "            )\n",
    "        )\n",
    "        yield(len(dates), date_trx, pov, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebc24179-d5c2-4cf0-afdf-e8b5a105c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_main_features = \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dca4b9b1-73fa-446d-81ea-223a18127df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(location_main_features, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88213f0e-2497-4f01-b735-f941102e4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# data_input = train.select(\"*\")\n",
    "# nodes_source = set(train.select(\"source\").distinct().toPandas()[\"source\"])\n",
    "# nodes_target = set(train.select(\"target\").distinct().toPandas()[\"target\"])\n",
    "# nodes_passthrough = nodes_source.intersection(nodes_target)\n",
    "\n",
    "# %run communities_global.ipynb\n",
    "\n",
    "# communities_as_source_features.to_parquet(f\"{location_main_features}/train_communities_as_source_features.parquet\")\n",
    "# communities_as_target_features.to_parquet(f\"{location_main_features}/train_communities_as_target_features.parquet\")\n",
    "# communities_as_passthrough_features.to_parquet(f\"{location_main_features}/train_communities_as_passthrough_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6ed3b9b-551e-4b78-a338-f7c800de7eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_as_source_features = pd.read_parquet(f\"{location_main_features}/train_communities_as_source_features.parquet\")\n",
    "communities_as_target_features = pd.read_parquet(f\"{location_main_features}/train_communities_as_target_features.parquet\")\n",
    "communities_as_passthrough_features = pd.read_parquet(f\"{location_main_features}/train_communities_as_passthrough_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a29581a8-b855-403c-adda-c3c0aa717dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_train = f\"{location_main_features}{os.sep}train{os.sep}\"\n",
    "location_validation = f\"{location_main_features}{os.sep}validation{os.sep}\"\n",
    "location_test = f\"{location_main_features}{os.sep}test{os.sep}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f45f4cf-663d-4fb9-8b90-4064bd2cf3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2022-09-01 in 15.0 minutes | (273775, 185) | num_days=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2022-09-02 in 21.0 minutes | (352061, 185) | num_days=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2022-09-03 in 9.0 minutes | (146548, 185) | num_days=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2022-09-04 in 9.0 minutes | (146618, 185) | num_days=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2022-09-05 in 13.0 minutes | (224048, 185) | num_days=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2022-09-06 in 13.0 minutes | (223096, 185) | num_days=6\n",
      "CPU times: user 22min 23s, sys: 1min 4s, total: 23min 28s\n",
      "Wall time: 1h 21min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    os.makedirs(location_train)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "st = time.time()\n",
    "for num_days, dt_trx, pov_df, window_df in get_windowed_datasets(train, train):\n",
    "    %run model_experiment_nested.ipynb\n",
    "    \n",
    "    all_features = all_features.join(\n",
    "        communities_as_source_features, how=\"left\", rsuffix=\"_dispense\"\n",
    "    ).join(\n",
    "        communities_as_target_features, how=\"left\", rsuffix=\"_sink\"\n",
    "    ).join(\n",
    "        communities_as_passthrough_features, how=\"left\", rsuffix=\"_passthrough\"\n",
    "    )\n",
    "    \n",
    "    pov_features_df = []\n",
    "    for k, v in pov_df[pov_df[\"source\"].isin(in_scope_sources)].groupby(\n",
    "        [\"source\", \"target\"]\n",
    "    ):\n",
    "        pov_features_df.append(get_pov_features(k, v))\n",
    "    pov_features = pd.DataFrame(pov_features_df)\n",
    "    \n",
    "    pov_features = pov_features.set_index(\"target\").join(\n",
    "        all_features, how=\"left\", rsuffix=\"_source\"\n",
    "    ).reset_index().set_index(\"source\").join(\n",
    "        all_features, how=\"left\", rsuffix=\"_target\"\n",
    "    ).reset_index()\n",
    "    \n",
    "    pov_features.to_parquet(f\"{location_train}{dt_trx}.parquet\")\n",
    "    print(f\"Processed {dt_trx} in {(time.time() - st) // 60} minutes | {all_features.shape} | {num_days=}\")\n",
    "    st = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "141b3793-9a00-4cd8-be99-cdab833b307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# data_input = train_validation.select(\"*\")\n",
    "# nodes_source = set(validation.select(\"source\").distinct().toPandas()[\"source\"])\n",
    "# nodes_target = set(validation.select(\"target\").distinct().toPandas()[\"target\"])\n",
    "# nodes_passthrough = nodes_source.intersection(nodes_target)\n",
    "\n",
    "# %run communities_global.ipynb\n",
    "\n",
    "# communities_as_source_features.to_parquet(f\"{location_main_features}/valid_communities_as_source_features.parquet\")\n",
    "# communities_as_target_features.to_parquet(f\"{location_main_features}/valid_communities_as_target_features.parquet\")\n",
    "# communities_as_passthrough_features.to_parquet(f\"{location_main_features}/valid_communities_as_passthrough_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c708126-960f-4e7e-9e06-c186136a7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_as_source_features = pd.read_parquet(f\"{location_main_features}/valid_communities_as_source_features.parquet\")\n",
    "communities_as_target_features = pd.read_parquet(f\"{location_main_features}/valid_communities_as_target_features.parquet\")\n",
    "communities_as_passthrough_features = pd.read_parquet(f\"{location_main_features}/valid_communities_as_passthrough_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd3defa4-a869-41ee-ac8b-107b5a5bb82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2022-09-06 in 13.0 minutes | (223618, 185) | num_days=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2022-09-07 in 13.0 minutes | (224078, 185) | num_days=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2022-09-08 in 13.0 minutes | (212250, 185) | num_days=3\n",
      "CPU times: user 11min 20s, sys: 33.9 s, total: 11min 54s\n",
      "Wall time: 40min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    os.makedirs(location_validation)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "st = time.time()\n",
    "for num_days, dt_trx, pov_df, window_df in get_windowed_datasets(validation, train_validation):\n",
    "    %run model_experiment_nested.ipynb\n",
    "\n",
    "    all_features = all_features.join(\n",
    "        communities_as_source_features, how=\"left\", rsuffix=\"_dispense\"\n",
    "    ).join(\n",
    "        communities_as_target_features, how=\"left\", rsuffix=\"_sink\"\n",
    "    ).join(\n",
    "        communities_as_passthrough_features, how=\"left\", rsuffix=\"_passthrough\"\n",
    "    )\n",
    "    \n",
    "    pov_features_df = []\n",
    "    for k, v in pov_df[pov_df[\"source\"].isin(in_scope_sources)].groupby(\n",
    "        [\"source\", \"target\"]\n",
    "    ):\n",
    "        pov_features_df.append(get_pov_features(k, v))\n",
    "    pov_features = pd.DataFrame(pov_features_df)\n",
    "\n",
    "    pov_features = pov_features.set_index(\"target\").join(\n",
    "        all_features, how=\"left\", rsuffix=\"_source\"\n",
    "    ).reset_index().set_index(\"source\").join(\n",
    "        all_features, how=\"left\", rsuffix=\"_target\"\n",
    "    ).reset_index()\n",
    "\n",
    "    pov_features.to_parquet(f\"{location_validation}{dt_trx}.parquet\")\n",
    "    print(f\"Processed {dt_trx} in {(time.time() - st) // 60} minutes | {all_features.shape} | {num_days=}\")\n",
    "    st = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70f22b9b-d7a0-49eb-b21a-2de53c3fe2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# data_input = data.select(\"*\")\n",
    "# nodes_source = set(test.select(\"source\").distinct().toPandas()[\"source\"])\n",
    "# nodes_target = set(test.select(\"target\").distinct().toPandas()[\"target\"])\n",
    "# nodes_passthrough = nodes_source.intersection(nodes_target)\n",
    "\n",
    "# %run communities_global.ipynb\n",
    "\n",
    "# communities_as_source_features.to_parquet(f\"{location_main_features}/test_communities_as_source_features.parquet\")\n",
    "# communities_as_target_features.to_parquet(f\"{location_main_features}/test_communities_as_target_features.parquet\")\n",
    "# communities_as_passthrough_features.to_parquet(f\"{location_main_features}/test_communities_as_passthrough_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1405712-3d8e-4fce-acda-77947d430b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_as_source_features = pd.read_parquet(f\"{location_main_features}/test_communities_as_source_features.parquet\")\n",
    "communities_as_target_features = pd.read_parquet(f\"{location_main_features}/test_communities_as_target_features.parquet\")\n",
    "communities_as_passthrough_features = pd.read_parquet(f\"{location_main_features}/test_communities_as_passthrough_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f1b67ba-00e0-48f3-bc9a-e917a789a3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2022-09-08 in 14.0 minutes | (224172, 185) | num_days=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2022-09-09 in 18.0 minutes | (345481, 185) | num_days=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2022-09-10 in 8.0 minutes | (147496, 185) | num_days=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2022-09-11 in 0.0 minutes | (338, 174) | num_days=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2022-09-12 in 0.0 minutes | (239, 174) | num_days=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2022-09-13 in 0.0 minutes | (146, 172) | num_days=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2022-09-14 in 0.0 minutes | (95, 170) | num_days=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2022-09-15 in 0.0 minutes | (37, 166) | num_days=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2022-09-16 in 0.0 minutes | (35, 165) | num_days=11\n",
      "Processed 2022-09-17 in 0.0 minutes | (21, 164) | num_days=11\n",
      "Processed 2022-09-18 in 0.0 minutes | (10, 159) | num_days=11\n",
      "CPU times: user 12min 59s, sys: 40.1 s, total: 13min 39s\n",
      "Wall time: 46min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    os.makedirs(location_test)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "st = time.time()\n",
    "for num_days, dt_trx, pov_df, window_df in get_windowed_datasets(test, data):\n",
    "    %run model_experiment_nested.ipynb\n",
    "\n",
    "    all_features = all_features.join(\n",
    "        communities_as_source_features, how=\"left\", rsuffix=\"_dispense\"\n",
    "    ).join(\n",
    "        communities_as_target_features, how=\"left\", rsuffix=\"_sink\"\n",
    "    ).join(\n",
    "        communities_as_passthrough_features, how=\"left\", rsuffix=\"_passthrough\"\n",
    "    )\n",
    "    \n",
    "    pov_features_df = []\n",
    "    for k, v in pov_df[pov_df[\"source\"].isin(in_scope_sources)].groupby(\n",
    "        [\"source\", \"target\"]\n",
    "    ):\n",
    "        pov_features_df.append(get_pov_features(k, v))\n",
    "    pov_features = pd.DataFrame(pov_features_df)\n",
    "\n",
    "    pov_features = pov_features.set_index(\"target\").join(\n",
    "        all_features, how=\"left\", rsuffix=\"_source\"\n",
    "    ).reset_index().set_index(\"source\").join(\n",
    "        all_features, how=\"left\", rsuffix=\"_target\"\n",
    "    ).reset_index()\n",
    "    \n",
    "    pov_features.to_parquet(f\"{location_test}{dt_trx}.parquet\")\n",
    "    print(f\"Processed {dt_trx} in {(time.time() - st) // 60} minutes | {all_features.shape} | {num_days=}\")\n",
    "    st = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1d6ed1b-8c5d-45b6-8ec7-17557a4ebb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.0\n"
     ]
    }
   ],
   "source": [
    "print((time.time() - start) // 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de28f56e-5dad-4d61-a090-a0923482456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(loc_main):\n",
    "    dfs = []\n",
    "    for location in glob(f\"{loc_main}{os.sep}*.parquet\"):\n",
    "        df_date = pd.read_parquet(location)\n",
    "        df_date.loc[:, \"date\"] = location.split(os.sep)[-1].split(\".\")[0]\n",
    "        dfs.append(df_date)\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c9c6375-7b12-4daf-a48d-aeb96caed993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "label_columns = [\"source\", \"target\", \"date\", \"is_laundering\"]\n",
    "\n",
    "train_features = load_dataset(location_train)\n",
    "train_features_labels = train_features.loc[:, label_columns].copy(deep=True)\n",
    "del train_features[\"source\"]\n",
    "del train_features[\"target\"]\n",
    "del train_features[\"date\"]\n",
    "del train_features[\"is_laundering\"]\n",
    "\n",
    "validation_features = load_dataset(location_validation)\n",
    "validation_features_labels = validation_features.loc[:, label_columns].copy(deep=True)\n",
    "validation_features = validation_features.loc[:, train_features.columns]\n",
    "\n",
    "test_features = load_dataset(location_test)\n",
    "test_features_labels = test_features.loc[:, label_columns].copy(deep=True)\n",
    "test_features = test_features.loc[:, train_features.columns]\n",
    "test_labels_orig = test.select([\"source\", \"target\", \"is_laundering\"]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1abac7d-644e-4b8f-b88d-394b5365afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bool_cols = [\n",
    "#     \"source_currency_aud\", \"source_currency_brl\", \"source_currency_btc\", \n",
    "#     \"source_currency_cad\", \"source_currency_chf\", \"source_currency_cny\", \n",
    "#     \"source_currency_gbp\", \"source_currency_ils\", \"source_currency_inr\", \n",
    "#     \"source_currency_jpy\", \"source_currency_mxn\", \"source_currency_rub\", \n",
    "#     \"format_Bitcoin\", \"format_Cash\", \"format_Cheque\", \"format_Credit Card\", \n",
    "#     \"format_Wire\"\n",
    "# ]\n",
    "# bool_cols_types = {c: bool for c in bool_cols}\n",
    "# for col in bool_cols:\n",
    "#     test_features.loc[:, col] = test_features[col].apply(\n",
    "#         lambda x: False if np.isnan(x) else x\n",
    "#     )\n",
    "# test_features = test_features.astype(bool_cols_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "46d781ea-ebbd-4cd1-a225-5318454aab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.experimental import enable_halving_search_cv\n",
    "# from sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV\n",
    "# from scipy.stats import randint\n",
    "\n",
    "# param_grid = {\n",
    "#     \"min_child_weight\": [1, 3, 5],\n",
    "#     \"subsample\": [0.5, 0.8, 1.0],\n",
    "#     \"colsample_bytree\": [0.5, 0.8, 1.0],\n",
    "#     \"learning_rate\": [0.01, 0.1, 0.3],\n",
    "#     \"scale_pos_weight\": [1, 5, 10],\n",
    "# }\n",
    "# param_distributions = {\n",
    "#     \"min_child_weight\": np.random.randint(1, 6),\n",
    "#     \"subsample\": 0.1 * np.random.randint(5, 11),\n",
    "#     \"colsample_bytree\": 0.1 * np.random.randint(5, 11),\n",
    "#     \"scale_pos_weight\": np.random.randint(1, 11),\n",
    "# }\n",
    "# # Create XGBoost classifier\n",
    "# model_xgb = xgb.XGBClassifier(\n",
    "#     n_estimators=100, objective=\"binary:logistic\", \n",
    "#     # eval_metric=f1_eval, disable_default_eval_metric=True,\n",
    "# )\n",
    "\n",
    "# # Perform halving grid search\n",
    "# halving_search = HalvingRandomSearchCV(\n",
    "#     estimator=model_xgb, param_distributions=param_distributions,  # param_grid=param_grid, \n",
    "#     cv=5, factor=3, resource=\"n_estimators\", max_resources=100, verbose=2\n",
    "# )\n",
    "# halving_search.fit(\n",
    "#     train_features, train_features_labels[\"is_laundering\"].values,\n",
    "#     eval_set=[\n",
    "#         # (train_features, train_features_labels[\"is_laundering\"].values), \n",
    "#         (validation_features, validation_features_labels[\"is_laundering\"].values)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Print best parameters\n",
    "# print(f\"Best parameters: {halving_search.best_params_}\")\n",
    "# print(f\"Best score: {halving_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62b81dd1-4aea-4f3b-8c42-8cfa417f187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = int(train_features_labels.shape[0] / train_features_labels[\"is_laundering\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dbf8d36-22b4-4974-9bc9-90b29063b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_eval(y, y_):\n",
    "    return 1 - f1_score(y, np.round(y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "468a2d6d-01b5-4a1c-b189-48043d027d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x, y, x_, y_):\n",
    "    model = xgb.XGBClassifier(\n",
    "        early_stopping_rounds=20, scale_pos_weight=10,\n",
    "        eval_metric=f1_eval, disable_default_eval_metric=True, num_parallel_tree=20, max_depth=6,\n",
    "        colsample_bytree=0.5, subsample=0.5,\n",
    "    )\n",
    "    model.fit(x, y, verbose=False, eval_set=[(x_, y_)])\n",
    "    print(f\"Best iteration: {model.best_iteration}\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a509768-9df2-44bb-8bf8-0d265252311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orig_prediction_data(labels_data, labels_orig, prediction_values):\n",
    "    labels_data = labels_data.copy(deep=True)\n",
    "    labels_orig = labels_orig.copy(deep=True)\n",
    "    labels_data.loc[:, \"predicted\"] = prediction_values\n",
    "    predictions_agg = labels_data.groupby([\"source\", \"target\"]).agg(\n",
    "        predicted=(\"predicted\", \"max\")\n",
    "    ).reset_index()\n",
    "    final_predictions = labels_orig.set_index([\"source\", \"target\"]).join(\n",
    "        predictions_agg.set_index([\"source\", \"target\"]), how=\"left\"\n",
    "    ).reset_index()\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "957dd8c5-a2ba-4726-afed-d5b1e1782912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 5\n",
      "\n",
      "aggregated 0.7231 0.6516\n",
      "final 75.39 69.03\n",
      "\n",
      "CPU times: user 11min 1s, sys: 7.43 s, total: 11min 9s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = train_model(\n",
    "    train_features, train_features_labels[\"is_laundering\"].values, \n",
    "    validation_features, validation_features_labels[\"is_laundering\"].values\n",
    ")\n",
    "y_test_predicted = model.predict(test_features)\n",
    "print(\n",
    "    \"aggregated\",\n",
    "    round(f1_score(test_features_labels[\"is_laundering\"], y_test_predicted), 4),\n",
    "    round(recall_score(test_features_labels[\"is_laundering\"], y_test_predicted), 4)\n",
    ")\n",
    "predictions_data = get_orig_prediction_data(\n",
    "    test_features_labels, test_labels_orig, y_test_predicted\n",
    ")\n",
    "f1_final = round(f1_score(predictions_data[\"is_laundering\"], predictions_data[\"predicted\"]) * 100, 2)\n",
    "print(\n",
    "    \"final\",\n",
    "    f1_final,\n",
    "    round(recall_score(predictions_data[\"is_laundering\"], predictions_data[\"predicted\"]) * 100, 2)\n",
    ")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df28b9b0-a844-4995-954f-d48cd2fe872b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Best iteration: 5\n",
      "\n",
      "75.14 69.39\n",
      "Fold 2\n",
      "Best iteration: 4\n",
      "\n",
      "75.46 68.18\n",
      "Fold 3\n",
      "Best iteration: 6\n",
      "\n",
      "74.91 68.91\n",
      "Fold 4\n",
      "Best iteration: 5\n",
      "\n",
      "74.98 69.33\n",
      "Fold 5\n",
      "Best iteration: 5\n",
      "\n",
      "76.58 69.81\n",
      "CPU times: user 49min 50s, sys: 30.1 s, total: 50min 20s\n",
      "Wall time: 5min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "CV_FOLD_PERC = 0.8\n",
    "N_FOLDS = 5\n",
    "\n",
    "f1_scores = []\n",
    "for fold in range(N_FOLDS):\n",
    "    print(\"Fold\", fold + 1)\n",
    "    x_train = train_features.sample(frac=CV_FOLD_PERC)\n",
    "    x_train_labels = x_train.loc[:, []].join(train_features_labels, how=\"left\")\n",
    "    x_validation = validation_features.sample(frac=CV_FOLD_PERC)\n",
    "    x_validation_labels = x_validation.loc[:, []].join(validation_features_labels, how=\"left\")\n",
    "    model = train_model(\n",
    "        x_train, x_train_labels[\"is_laundering\"].values, \n",
    "        x_validation, x_validation_labels[\"is_laundering\"].values\n",
    "    )\n",
    "    y_test_predicted = model.predict(test_features)\n",
    "    predictions_data = get_orig_prediction_data(\n",
    "        test_features_labels, test_labels_orig, y_test_predicted\n",
    "    )\n",
    "    f1_cv = f1_score(predictions_data[\"is_laundering\"], predictions_data[\"predicted\"]) * 100\n",
    "    print(\n",
    "        round(f1_cv, 2),\n",
    "        round(recall_score(predictions_data[\"is_laundering\"], predictions_data[\"predicted\"]) * 100, 2)\n",
    "    )\n",
    "    f1_scores.append(f1_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "838ac53f-5cba-4999-a8e5-b04224da8f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.39 Â±0.61\n"
     ]
    }
   ],
   "source": [
    "print(f\"{f1_final} Â±{round(np.std(f1_scores), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5057b55-d11c-4fff-9c72-7ec354bd9d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
