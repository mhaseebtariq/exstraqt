{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6844f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "from glob import glob\n",
    "from datetime import timedelta, datetime\n",
    "from itertools import combinations\n",
    "\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "\n",
    "import settings as s\n",
    "from common import create_workload_for_multi_proc\n",
    "from communities import get_communities_multi_proc\n",
    "from features import get_features_multi_proc, get_pov_features\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c7f1e2-c0f3-4464-b223-ddd8c6fef69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/23 13:23:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "config = [\n",
    "    (\"spark.driver.memory\", \"16g\"),\n",
    "    (\"spark.worker.memory\", \"16g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"16g\"),\n",
    "]\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(config))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e69899e-e6a0-4f84-a288-737de0dd3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fdc571c-b31c-4970-8f32-a9955cceb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 7\n",
    "TRAIN_PERC = 0.6\n",
    "VALIDATION_PERC = 0.2\n",
    "TEST_PERC = 0.2\n",
    "\n",
    "NUM_PROCS = 10\n",
    "\n",
    "assert(sum([TRAIN_PERC, VALIDATION_PERC, TEST_PERC]) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c35a497-3f7f-498a-8a5a-da1a78d1f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet(s.STAGED_DATA_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67e38160-dd36-48de-aec6-94e07bf8cc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal 2025934\n",
      "extreme 192\n",
      "cuttoff 76.0\n"
     ]
    }
   ],
   "source": [
    "data_ud = data.select(\"source\", \"target\").union(\n",
    "    data.select(sf.col(\"target\").alias(\"source\"), sf.col(\"source\").alias(\"target\"))\n",
    ")\n",
    "top_nodes = data_ud.groupby(\"source\").agg(sf.countDistinct(\"target\").alias(\"count\")).toPandas()\n",
    "cutoff_extreme = np.percentile(top_nodes[\"count\"], 99.99)\n",
    "nodes_extreme = top_nodes[top_nodes[\"count\"] > cutoff_extreme][\"source\"].tolist()\n",
    "nodes_normal = list(set(top_nodes[\"source\"].tolist()) - set(nodes_extreme))\n",
    "print(\"normal\", len(nodes_normal))\n",
    "print(\"extreme\", len(nodes_extreme))\n",
    "print(\"cuttoff\", cutoff_extreme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2103dcc9-a709-4d42-b17a-9f403aa684ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-16 23:59:00\n"
     ]
    }
   ],
   "source": [
    "# The last few days only contain incomplete data\n",
    "trx_count_per_day = data.groupby(sf.to_date(\"timestamp\").alias(\"date\")).count().toPandas()\n",
    "trx_count_per_day = trx_count_per_day.sort_values(\"date\").set_index(\"date\")\n",
    "mean_per_day = np.mean(trx_count_per_day[\"count\"])\n",
    "mean_per_day_ratio = trx_count_per_day[\"count\"] / mean_per_day\n",
    "complete_data_present_till = max(mean_per_day_ratio[mean_per_day_ratio > 0.1].index)\n",
    "complete_data_present_till = data.where(sf.to_date(\"timestamp\") == complete_data_present_till).select(\n",
    "    sf.max(\"timestamp\").alias(\"x\")\n",
    ").collect()[0][\"x\"]\n",
    "print(complete_data_present_till)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b936c12-027a-4cec-8360-76fb92def8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "trx_ids_sorted = data.sort(\"timestamp\").select(\"transaction_id\").toPandas()[\"transaction_id\"].values\n",
    "trx_count = len(trx_ids_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70a13395-4873-4612-b464-1e8103b5ff75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/23 13:26:08 WARN TaskSetManager: Stage 17 contains a task of very large size (10591 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18734115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/23 13:26:39 WARN TaskSetManager: Stage 22 contains a task of very large size (3579 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6244705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/23 13:27:09 WARN TaskSetManager: Stage 27 contains a task of very large size (3579 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6244705\n"
     ]
    }
   ],
   "source": [
    "last_train_index = int(np.floor(trx_count * TRAIN_PERC))\n",
    "last_validation_index = last_train_index + int(np.floor(trx_count * VALIDATION_PERC))\n",
    "train_indexes = trx_ids_sorted[:last_train_index]\n",
    "validation_indexes = trx_ids_sorted[last_train_index:last_validation_index]\n",
    "test_indexes = trx_ids_sorted[last_validation_index:]\n",
    "\n",
    "train_indexes = spark.createDataFrame(\n",
    "    pd.DataFrame(train_indexes, columns=[\"transaction_id\"])\n",
    ").repartition(1).cache()\n",
    "print(train_indexes.count())\n",
    "validation_indexes = spark.createDataFrame(\n",
    "    pd.DataFrame(validation_indexes, columns=[\"transaction_id\"])\n",
    ").repartition(1).cache()\n",
    "print(validation_indexes.count())\n",
    "test_indexes = spark.createDataFrame(\n",
    "    pd.DataFrame(test_indexes, columns=[\"transaction_id\"])\n",
    ").repartition(1).cache()\n",
    "print(test_indexes.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d5837a4-1085-4d7e-82e9-c9f61afa24c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ")\n",
    "validation = validation_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ")\n",
    "test = test_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ")\n",
    "train_validation = train.union(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9a31321-bf63-4ed6-9323-171b71a161b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pandas(df):\n",
    "    df.write.parquet(\"temp.parquet\", mode=\"overwrite\")\n",
    "    df = pd.read_parquet(\"temp.parquet\")\n",
    "    # Because of tz discrepancy\n",
    "    df.loc[:, \"timestamp\"] += timedelta(hours=2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b9aa236-047f-41c9-b7c3-a78b5a3a02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windowed_datasets(data_dates, data_input):\n",
    "    dates = data_dates.select(sf.to_date(\"timestamp\").alias(\"x\")).distinct().collect()\n",
    "    dates = sorted([x[\"x\"] for x in dates])\n",
    "    for date_trx in dates:\n",
    "        datetime_trx_start = datetime.combine(date_trx, datetime.min.time())\n",
    "        datetime_trx_end = datetime.combine(date_trx, datetime.max.time())\n",
    "        left_start = datetime_trx_start - timedelta(WINDOW_SIZE)\n",
    "        right_end = datetime_trx_end + timedelta(WINDOW_SIZE)\n",
    "        pov = get_pandas(\n",
    "            data_input.where(\n",
    "                (data_input[\"timestamp\"] >= datetime_trx_start) & (data_input[\"timestamp\"] <= datetime_trx_end)\n",
    "            )\n",
    "        )\n",
    "        window = get_pandas(\n",
    "            data_input.where(\n",
    "                (data_input[\"timestamp\"] >= left_start) & (data_input[\"timestamp\"] <= right_end)\n",
    "            )\n",
    "        )\n",
    "        yield(len(dates), date_trx, pov, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb99d4c-0be6-4c79-bff1-4da1152d22f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_main_features = os.path.join(\"features\", s.OUTPUT_POSTFIX.lstrip(\"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dca4b9b1-73fa-446d-81ea-223a18127df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(location_main_features, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a29581a8-b855-403c-adda-c3c0aa717dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_train = f\"{location_main_features}{os.sep}train{os.sep}\"\n",
    "location_validation = f\"{location_main_features}{os.sep}validation{os.sep}\"\n",
    "location_test = f\"{location_main_features}{os.sep}test{os.sep}\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(location_main_features)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88213f0e-2497-4f01-b735-f941102e4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# data_input = train.select(\"*\")\n",
    "# nodes_source = set(train.select(\"source\").distinct().toPandas()[\"source\"])\n",
    "# nodes_target = set(train.select(\"target\").distinct().toPandas()[\"target\"])\n",
    "# nodes_passthrough = nodes_source.intersection(nodes_target)\n",
    "\n",
    "# %run communities_global.ipynb\n",
    "\n",
    "# communities_as_source_features.to_parquet(f\"{location_main_features}/train_communities_as_source_features.parquet\")\n",
    "# communities_as_target_features.to_parquet(f\"{location_main_features}/train_communities_as_target_features.parquet\")\n",
    "# communities_as_passthrough_features.to_parquet(f\"{location_main_features}/train_communities_as_passthrough_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6ed3b9b-551e-4b78-a338-f7c800de7eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# communities_as_source_features = pd.read_parquet(f\"{location_main_features}/train_communities_as_source_features.parquet\")\n",
    "# communities_as_target_features = pd.read_parquet(f\"{location_main_features}/train_communities_as_target_features.parquet\")\n",
    "# communities_as_passthrough_features = pd.read_parquet(f\"{location_main_features}/train_communities_as_passthrough_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f45f4cf-663d-4fb9-8b90-4064bd2cf3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# try:\n",
    "#     os.makedirs(location_train)\n",
    "# except FileExistsError:\n",
    "#     pass\n",
    "\n",
    "# st = time.time()\n",
    "# for num_days, dt_trx, pov_df, window_df in get_windowed_datasets(train, train):\n",
    "#     %run model_experiment_nested_hm.ipynb\n",
    "    \n",
    "#     all_features = all_features.join(\n",
    "#         communities_as_source_features, how=\"left\", rsuffix=\"_dispense\"\n",
    "#     ).join(\n",
    "#         communities_as_target_features, how=\"left\", rsuffix=\"_sink\"\n",
    "#     ).join(\n",
    "#         communities_as_passthrough_features, how=\"left\", rsuffix=\"_passthrough\"\n",
    "#     )\n",
    "    \n",
    "#     pov_features_df = []\n",
    "#     for k, v in pov_df[pov_df[\"source\"].isin(in_scope_sources)].groupby(\n",
    "#         [\"source\", \"target\"]\n",
    "#     ):\n",
    "#         pov_features_df.append(get_pov_features(k, v))\n",
    "#     pov_features = pd.DataFrame(pov_features_df)\n",
    "    \n",
    "#     pov_features = pov_features.set_index(\"target\").join(\n",
    "#         all_features, how=\"left\", rsuffix=\"_source\"\n",
    "#     ).reset_index().set_index(\"source\").join(\n",
    "#         all_features, how=\"left\", rsuffix=\"_target\"\n",
    "#     ).reset_index()\n",
    "    \n",
    "#     pov_features.to_parquet(f\"{location_train}{dt_trx}.parquet\")\n",
    "#     print(f\"Processed {dt_trx} in {(time.time() - st) // 60} minutes | {all_features.shape} | {num_days=}\")\n",
    "#     st = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f436e588-e54b-4c63-a651-8251e9e21934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# grouped = window_df.groupby([\"source\", \"target\"]).agg(amount=(\"amount\", \"sum\")).reset_index()\n",
    "# grouped_rev = grouped.copy(deep=True).rename(columns={\"target\": \"source\", \"source\": \"target\"})\n",
    "# grouped_ud = pd.concat([\n",
    "#     grouped,\n",
    "#     grouped_rev\n",
    "# ], ignore_index=True)\n",
    "# nodes = set(grouped_ud[\"target\"].unique().tolist()) - set(top_nodes)\n",
    "# nodes = list(nodes)\n",
    "# random.shuffle(nodes)\n",
    "# df_left = grouped_ud.set_index(\"target\")\n",
    "# df_right = grouped_ud.set_index(\"source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b518e23-978c-4dbc-9ebd-b518b8f141dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# comms = []\n",
    "# for chunk in [[x] for x in top_nodes] + list(np.array_split(nodes, np.ceil(len(nodes) / 100_000))):\n",
    "#     joined = df_left.loc[chunk, :].join(\n",
    "#         df_right, how=\"inner\", lsuffix=\"_left\"\n",
    "#     ).reset_index(drop=True)\n",
    "#     joined = joined.loc[joined[\"source\"] != joined[\"target\"], :]\n",
    "#     joined.loc[:, \"amount\"] = joined[[\"amount_left\", \"amount\"]].min(axis=1)\n",
    "#     del joined[\"amount_left\"]\n",
    "#     joined = joined.groupby([\"source\", \"target\"]).agg(amount=(\"amount\", \"sum\")).reset_index()\n",
    "#     joined = joined.sort_values(\"amount\", ascending=False).reset_index(drop=True)\n",
    "#     joined = joined.groupby(\"source\").head(100).reset_index(drop=True)\n",
    "#     comms.append(joined.groupby(\"source\").agg(nodes=(\"target\", list)))\n",
    "#     print(len(chunk), comms[-1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37e0cbec-c05f-4516-bc53-c149086da93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped = window_df.groupby([\"source\", \"target\"]).agg(sf.sum(\"amount\").alias(\"amount\"))\n",
    "# grouped.cache()\n",
    "# grouped.count()\n",
    "# grouped_ud = grouped.union(\n",
    "#     grouped.select(sf.col(\"target\").alias(\"source\"), sf.col(\"source\").alias(\"target\"), \"amount\")\n",
    "# )\n",
    "# grouped_ud_left = grouped_ud.select(*[sf.col(x).alias(f\"left_{x}\") for x in grouped_ud_left.columns])\n",
    "# joined = grouped_ud_left.join(\n",
    "#     grouped_ud,\n",
    "#     (grouped_ud_left[\"left_target\"] == grouped_ud[\"source\"]) &\n",
    "#     (grouped_ud_left[\"left_source\"] != grouped_ud[\"target\"])\n",
    "# ).select(\n",
    "#     sf.col(\"left_source\").alias(\"source\"), \"target\", \n",
    "#     sf.least(\"left_amount\", \"amount\").alias(\"amount\")\n",
    "# )\n",
    "# window = Window.partitionBy(joined[\"source\"]).orderBy(joined[\"amount\"].desc())\n",
    "# joined = joined.select(\n",
    "#     \"*\", sf.row_number().over(window).alias(\"row_number\")\n",
    "# ).where(sf.col(\"row_number\") <= 100).cache()\n",
    "# joined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "141b3793-9a00-4cd8-be99-cdab833b307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# data_input = train_validation.select(\"*\")\n",
    "# nodes_source = set(validation.select(\"source\").distinct().toPandas()[\"source\"])\n",
    "# nodes_target = set(validation.select(\"target\").distinct().toPandas()[\"target\"])\n",
    "# nodes_passthrough = nodes_source.intersection(nodes_target)\n",
    "\n",
    "# %run communities_global.ipynb\n",
    "\n",
    "# communities_as_source_features.to_parquet(f\"{location_main_features}/valid_communities_as_source_features.parquet\")\n",
    "# communities_as_target_features.to_parquet(f\"{location_main_features}/valid_communities_as_target_features.parquet\")\n",
    "# communities_as_passthrough_features.to_parquet(f\"{location_main_features}/valid_communities_as_passthrough_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c708126-960f-4e7e-9e06-c186136a7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_as_source_features = pd.read_parquet(f\"{location_main_features}/valid_communities_as_source_features.parquet\")\n",
    "communities_as_target_features = pd.read_parquet(f\"{location_main_features}/valid_communities_as_target_features.parquet\")\n",
    "communities_as_passthrough_features = pd.read_parquet(f\"{location_main_features}/valid_communities_as_passthrough_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3defa4-a869-41ee-ac8b-107b5a5bb82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    os.makedirs(location_validation)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "st = time.time()\n",
    "for num_days, dt_trx, pov_df, window_df in get_windowed_datasets(validation, train_validation):\n",
    "    %run model_experiment_nested_hm.ipynb\n",
    "\n",
    "    all_features = all_features.join(\n",
    "        communities_as_source_features, how=\"left\", rsuffix=\"_dispense\"\n",
    "    ).join(\n",
    "        communities_as_target_features, how=\"left\", rsuffix=\"_sink\"\n",
    "    ).join(\n",
    "        communities_as_passthrough_features, how=\"left\", rsuffix=\"_passthrough\"\n",
    "    )\n",
    "    \n",
    "    pov_features_df = []\n",
    "    for k, v in pov_df[pov_df[\"source\"].isin(in_scope_sources)].groupby(\n",
    "        [\"source\", \"target\"]\n",
    "    ):\n",
    "        pov_features_df.append(get_pov_features(k, v))\n",
    "    pov_features = pd.DataFrame(pov_features_df)\n",
    "\n",
    "    pov_features = pov_features.set_index(\"target\").join(\n",
    "        all_features, how=\"left\", rsuffix=\"_source\"\n",
    "    ).reset_index().set_index(\"source\").join(\n",
    "        all_features, how=\"left\", rsuffix=\"_target\"\n",
    "    ).reset_index()\n",
    "    pov_features.to_parquet(f\"{location_validation}{dt_trx}.parquet\")\n",
    "    print(f\"Processed {dt_trx} in {(time.time() - st) // 60} minutes | {all_features.shape} | {num_days=}\")\n",
    "    st = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f22b9b-d7a0-49eb-b21a-2de53c3fe2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# data_input = data.select(\"*\")\n",
    "# nodes_source = set(test.select(\"source\").distinct().toPandas()[\"source\"])\n",
    "# nodes_target = set(test.select(\"target\").distinct().toPandas()[\"target\"])\n",
    "# nodes_passthrough = nodes_source.intersection(nodes_target)\n",
    "\n",
    "# %run communities_global.ipynb\n",
    "\n",
    "# communities_as_source_features.to_parquet(f\"{location_main_features}/test_communities_as_source_features.parquet\")\n",
    "# communities_as_target_features.to_parquet(f\"{location_main_features}/test_communities_as_target_features.parquet\")\n",
    "# communities_as_passthrough_features.to_parquet(f\"{location_main_features}/test_communities_as_passthrough_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1405712-3d8e-4fce-acda-77947d430b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_as_source_features = pd.read_parquet(f\"{location_main_features}/test_communities_as_source_features.parquet\")\n",
    "communities_as_target_features = pd.read_parquet(f\"{location_main_features}/test_communities_as_target_features.parquet\")\n",
    "communities_as_passthrough_features = pd.read_parquet(f\"{location_main_features}/test_communities_as_passthrough_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b67ba-00e0-48f3-bc9a-e917a789a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    os.makedirs(location_test)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "first_test_timestamp = test.select(sf.min(\"timestamp\").alias(\"x\")).collect()[0][\"x\"]\n",
    "\n",
    "st = time.time()\n",
    "for num_days, dt_trx, pov_df, window_df in get_windowed_datasets(test, data):\n",
    "    # Just to (double) make sure\n",
    "    pov_df = pov_df.loc[pov_df[\"timestamp\"] >= first_test_timestamp, :].copy(deep=True)\n",
    "    \n",
    "    %run model_experiment_nested_hm.ipynb\n",
    "\n",
    "    all_features = all_features.join(\n",
    "        communities_as_source_features, how=\"left\", rsuffix=\"_dispense\"\n",
    "    ).join(\n",
    "        communities_as_target_features, how=\"left\", rsuffix=\"_sink\"\n",
    "    ).join(\n",
    "        communities_as_passthrough_features, how=\"left\", rsuffix=\"_passthrough\"\n",
    "    )\n",
    "    \n",
    "    pov_features_df = []\n",
    "    for k, v in pov_df[pov_df[\"source\"].isin(in_scope_sources)].groupby(\n",
    "        [\"source\", \"target\"]\n",
    "    ):\n",
    "        pov_features_df.append(get_pov_features(k, v))\n",
    "    pov_features = pd.DataFrame(pov_features_df)\n",
    "\n",
    "    pov_features = pov_features.set_index(\"target\").join(\n",
    "        all_features, how=\"left\", rsuffix=\"_source\"\n",
    "    ).reset_index().set_index(\"source\").join(\n",
    "        all_features, how=\"left\", rsuffix=\"_target\"\n",
    "    ).reset_index()\n",
    "    \n",
    "    pov_features.to_parquet(f\"{location_test}{dt_trx}.parquet\")\n",
    "    print(f\"Processed {dt_trx} in {(time.time() - st) // 60} minutes | {all_features.shape} | {num_days=}\")\n",
    "    st = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d6ed1b-8c5d-45b6-8ec7-17557a4ebb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((time.time() - start) // 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28f56e-5dad-4d61-a090-a0923482456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(loc_main):\n",
    "    dfs = []\n",
    "    for location in glob(f\"{loc_main}{os.sep}*.parquet\"):\n",
    "        df_date = pd.read_parquet(location)\n",
    "        df_date.loc[:, \"date\"] = location.split(os.sep)[-1].split(\".\")[0]\n",
    "        dfs.append(df_date)\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c6375-7b12-4daf-a48d-aeb96caed993",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = [\"source\", \"target\", \"date\", \"is_laundering\"]\n",
    "\n",
    "train_features = load_dataset(location_train)\n",
    "train_features_labels = train_features.loc[:, label_columns].copy(deep=True)\n",
    "del train_features[\"source\"]\n",
    "del train_features[\"target\"]\n",
    "del train_features[\"date\"]\n",
    "del train_features[\"is_laundering\"]\n",
    "\n",
    "validation_features = load_dataset(location_validation)\n",
    "validation_features_labels = validation_features.loc[:, label_columns].copy(deep=True)\n",
    "# TODO: This is not ideal\n",
    "for missing in set(train_features.columns) - set(validation_features.columns):\n",
    "    del train_features[missing]\n",
    "    print(missing)\n",
    "validation_features = validation_features.loc[:, train_features.columns]\n",
    "\n",
    "test_features = load_dataset(location_test)\n",
    "test_features_labels = test_features.loc[:, label_columns].copy(deep=True)\n",
    "# TODO: This is not ideal\n",
    "for missing in set(train_features.columns) - set(test_features.columns):\n",
    "    del train_features[missing]\n",
    "    print(missing)\n",
    "test_features = test_features.loc[:, train_features.columns]\n",
    "test_labels_orig = test.select([\"source\", \"target\", \"is_laundering\"]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b81dd1-4aea-4f3b-8c42-8cfa417f187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = int(train_features_labels.shape[0] / train_features_labels[\"is_laundering\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf8d36-22b4-4974-9bc9-90b29063b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_eval(y, y_):\n",
    "    return 1 - f1_score(y, np.round(y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a2d6d-01b5-4a1c-b189-48043d027d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For HI\n",
    "# def train_model(x, y, x_, y_):\n",
    "#     model = xgb.XGBClassifier(\n",
    "#         early_stopping_rounds=20, scale_pos_weight=10,\n",
    "#         eval_metric=f1_eval, disable_default_eval_metric=True, num_parallel_tree=20, max_depth=6,\n",
    "#         colsample_bytree=0.5, subsample=0.5,\n",
    "#     )\n",
    "#     model.fit(x, y, verbose=False, eval_set=[(x_, y_)])\n",
    "#     print(f\"Best iteration: {model.best_iteration}\\n\")\n",
    "#     return model\n",
    "\n",
    "\n",
    "# For LI\n",
    "def train_model(x, y, x_, y_):\n",
    "    model = xgb.XGBClassifier(\n",
    "        early_stopping_rounds=20, scale_pos_weight=3,\n",
    "        eval_metric=f1_eval, disable_default_eval_metric=True, num_parallel_tree=20, max_depth=6,\n",
    "        colsample_bytree=0.5, subsample=0.5,\n",
    "    )\n",
    "    model.fit(x, y, verbose=False, eval_set=[(x_, y_)])\n",
    "    print(f\"Best iteration: {model.best_iteration}\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a509768-9df2-44bb-8bf8-0d265252311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orig_prediction_data(labels_data, labels_orig, prediction_values):\n",
    "    labels_data = labels_data.copy(deep=True)\n",
    "    labels_orig = labels_orig.copy(deep=True)\n",
    "    labels_data.loc[:, \"predicted\"] = prediction_values\n",
    "    predictions_agg = labels_data.groupby([\"source\", \"target\"]).agg(\n",
    "        predicted=(\"predicted\", \"max\")\n",
    "    ).reset_index()\n",
    "    final_predictions = labels_orig.set_index([\"source\", \"target\"]).join(\n",
    "        predictions_agg.set_index([\"source\", \"target\"]), how=\"left\"\n",
    "    ).reset_index()\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957dd8c5-a2ba-4726-afed-d5b1e1782912",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = train_model(\n",
    "    train_features, train_features_labels[\"is_laundering\"].values, \n",
    "    validation_features, validation_features_labels[\"is_laundering\"].values\n",
    ")\n",
    "y_test_predicted = model.predict(test_features)\n",
    "print(\n",
    "    \"aggregated\",\n",
    "    round(f1_score(test_features_labels[\"is_laundering\"], y_test_predicted) * 100, 2),\n",
    "    round(recall_score(test_features_labels[\"is_laundering\"], y_test_predicted) * 100, 2)\n",
    ")\n",
    "predictions_data = get_orig_prediction_data(\n",
    "    test_features_labels, test_labels_orig, y_test_predicted\n",
    ")\n",
    "f1_final = round(f1_score(predictions_data[\"is_laundering\"], predictions_data[\"predicted\"]) * 100, 2)\n",
    "print(\n",
    "    \"final\",\n",
    "    f1_final,\n",
    "    round(recall_score(predictions_data[\"is_laundering\"], predictions_data[\"predicted\"]) * 100, 2)\n",
    ")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df28b9b0-a844-4995-954f-d48cd2fe872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "CV_FOLD_PERC = 0.8\n",
    "N_FOLDS = 5\n",
    "\n",
    "f1_scores = []\n",
    "for fold in range(N_FOLDS):\n",
    "    print(\"Fold\", fold + 1)\n",
    "    x_train = train_features.sample(frac=CV_FOLD_PERC)\n",
    "    x_train_labels = x_train.loc[:, []].join(train_features_labels, how=\"left\")\n",
    "    x_validation = validation_features.sample(frac=CV_FOLD_PERC)\n",
    "    x_validation_labels = x_validation.loc[:, []].join(validation_features_labels, how=\"left\")\n",
    "    model = train_model(\n",
    "        x_train, x_train_labels[\"is_laundering\"].values, \n",
    "        x_validation, x_validation_labels[\"is_laundering\"].values\n",
    "    )\n",
    "    y_test_predicted = model.predict(test_features)\n",
    "    predictions_data = get_orig_prediction_data(\n",
    "        test_features_labels, test_labels_orig, y_test_predicted\n",
    "    )\n",
    "    f1_cv = f1_score(predictions_data[\"is_laundering\"], predictions_data[\"predicted\"]) * 100\n",
    "    print(\n",
    "        round(f1_cv, 2),\n",
    "        round(recall_score(predictions_data[\"is_laundering\"], predictions_data[\"predicted\"]) * 100, 2)\n",
    "    )\n",
    "    f1_scores.append(f1_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ac53f-5cba-4999-a8e5-b04224da8f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{f1_final} Â±{round(np.std(f1_scores), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5057b55-d11c-4fff-9c72-7ec354bd9d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
